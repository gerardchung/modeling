<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Ordinary Least Squares (OLS) Estimation | Statistical Modeling and Computation for Educational Scientists" />
<meta property="og:type" content="book" />


<meta property="og:description" content="EPsy 8251 and 8252 Notes" />
<meta name="github-repo" content="zief0002/modeling" />

<meta name="author" content="Andrew Zieffler" />

<meta name="date" content="2020-05-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="EPsy 8251 and 8252 Notes">

<title>Ordinary Least Squares (OLS) Estimation | Statistical Modeling and Computation for Educational Scientists</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="style/style.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/navbar.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#foreword">Foreword</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="slrd.html#slrd">Simple Linear Regression—Description</a></li>
<li><a href="ordinary-least-squares-ols-estimation.html#ordinary-least-squares-ols-estimation">Ordinary Least Squares (OLS) Estimation</a></li>
<li><a href="cor.html#cor">Correlation and Standardized Regression</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="ordinary-least-squares-ols-estimation" class="section level1">
<h1>Ordinary Least Squares (OLS) Estimation</h1>
<p>In this set of notes, you will learn how the coefficients from the fitted regression equation are estimated from the data. Recall that in the previous set of notes, we used the <a href="https://raw.githubusercontent.com/zief0002/modeling/master/data/riverview.csv">riverview.csv</a> data to examine whether education level is related to income (see the <a href="http://zief0002.github.io/epsy-8251/codebooks/riverview.html">data codebook</a>). To begin, we will load several libraries and import the data into an object called <code>city</code>. We will also fit a model by regressing income on education level and storing those results in an object called <code>lm.1</code>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" title="1"><span class="co"># Load libraries</span></a>
<a class="sourceLine" id="cb18-2" title="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb18-3" title="3"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb18-4" title="4"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb18-5" title="5"></a>
<a class="sourceLine" id="cb18-6" title="6"><span class="co"># Read in data</span></a>
<a class="sourceLine" id="cb18-7" title="7">city =<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&quot;https://raw.githubusercontent.com/zief0002/modeling/master/data/riverview.csv&quot;</span>)</a>
<a class="sourceLine" id="cb18-8" title="8"><span class="kw">head</span>(city)</a></code></pre></div>
<pre><code># A tibble: 6 x 6
  education income seniority gender  male party      
      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      
1         8   37.4         7 male       1 Democrat   
2         8   26.4         9 female     0 Independent
3        10   47.0        14 male       1 Democrat   
4        10   34.2        16 female     0 Independent
5        10   25.5         1 female     0 Republican 
6        12   46.5        11 female     0 Democrat   </code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># Fit regression model</span></a>
<a class="sourceLine" id="cb20-2" title="2">lm<span class="fl">.1</span> =<span class="st"> </span><span class="kw">lm</span>(income <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>education, <span class="dt">data =</span> city)</a>
<a class="sourceLine" id="cb20-3" title="3">lm<span class="fl">.1</span></a></code></pre></div>
<pre><code>
Call:
lm(formula = income ~ 1 + education, data = city)

Coefficients:
(Intercept)    education  
     11.321        2.651  </code></pre>
<p>The fitted regression equation is</p>
<p><span class="math display">\[
\hat{\mathrm{Income}_i} = 11.321 + 2.651(\mathrm{Education~Level}_i)
\]</span></p>
<p><br /></p>
<div id="ordinary-least-squares-estimation" class="section level2">
<h2>Ordinary Least Squares Estimation</h2>
<p>How does R determine the coefficient values of <span class="math inline">\(\hat{\beta}_0=11.321\)</span> and <span class="math inline">\(\hat{\beta}_1=2.651\)</span>? These values are estimated from the data using a method called <em>Ordinary Least Squares</em> (OLS). To understand how OLS works, consider the following toy data set of five observations:</p>
<table style="width:40%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-3">Table 1: </span>Toy data set with predictor (X) and outcome (Y) for five observations.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
63
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
44
</td>
</tr>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
40
</td>
</tr>
<tr>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
68
</td>
</tr>
<tr>
<td style="text-align:center;">
20
</td>
<td style="text-align:center;">
25
</td>
</tr>
</tbody>
</table>
<p>Which of the following two models fits these data better?</p>
<ul>
<li><strong>Model A:</strong> <span class="math inline">\(~~\hat{Y_i} = 28 + 0.8(X_i)\)</span></li>
<li><strong>Model B:</strong> <span class="math inline">\(~~\hat{Y_i} = 20 + 1(X_i)\)</span></li>
</ul>
<p>We could plot the data and both lines and try to determine which seems to fit better.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="05-ols-estimation_files/figure-html/unnamed-chunk-4-1.png" alt="Scatterplot of the observed toy data and the OLS fitted regression line for two models." width="45%" /><img src="05-ols-estimation_files/figure-html/unnamed-chunk-4-2.png" alt="Scatterplot of the observed toy data and the OLS fitted regression line for two models." width="45%" />
<p class="caption">
Figure 9: Scatterplot of the observed toy data and the OLS fitted regression line for two models.
</p>
</div>
<p><br /></p>
</div>
<div id="datamodel-fit" class="section level2">
<h2>Data–Model Fit</h2>
<p>In this case, the lines are similar and it is difficult to make a determination of which fits the data better by eyeballing the two plots. Instead of guessing which model fits better, we can actually quantify the fit to the data by computing the residuals (errors) for each model and then compare both sets of residuals; larger errors indicate a worse fitting model (i.e., more misfit to the data).</p>
<p>Remember, to compute the residuals, we will first need to compute the predicted value (<span class="math inline">\(\hat{Y}_i\)</span>) for each of the five observations for both models.</p>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-5">Table 2: </span>Observed values, predicted values and residuals for Model A.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{Y_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
63
</td>
<td style="text-align:center;">
52
</td>
<td style="text-align:center;">
11
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
36
</td>
<td style="text-align:center;">
8
</td>
</tr>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
52
</td>
<td style="text-align:center;">
-12
</td>
</tr>
<tr>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
20
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
-19
</td>
</tr>
</tbody>
</table>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-6">Table 3: </span>Observed values, predicted values and residuals for Model B.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{Y_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
63
</td>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
13
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
14
</td>
</tr>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
-10
</td>
</tr>
<tr>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
70
</td>
<td style="text-align:center;">
-2
</td>
</tr>
<tr>
<td style="text-align:center;">
20
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
-15
</td>
</tr>
</tbody>
</table>
<p>Eyeballing the numeric values of the residuals is also problematic. The size of the residuals is similar for both Models. Also, the eyeballing method would be impractical for larger datasets. So, we have to further quantify the model fit (or misfit). The way we do that in practice is to consider the <em>total</em> amount of error across all the observations. Unfortunately, we cannot just sum the residuals to get the total because some of our residuals are negative and some are positive. To alleviate this problem, we first square the residuals, then we sum them.</p>
<p><span class="math display">\[
\begin{split}
\mathrm{Total~Error} &amp;= \sum\hat{\epsilon}_i^2 \\
&amp;= \sum \left( Y_i - \hat{Y}_i\right)^2
\end{split}
\]</span></p>
<p>This is called a <em>sum of squared residuals</em> or <em>sum of squared error</em> (SSE; good name, isn’t it). Computing the squared residuals for Model A and Model B we get:</p>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-7">Table 4: </span>Observed values, predicted values, residuals, and squared residuals for Model A.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{Y_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
63
</td>
<td style="text-align:center;">
52
</td>
<td style="text-align:center;">
11
</td>
<td style="text-align:center;">
121
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
36
</td>
<td style="text-align:center;">
8
</td>
<td style="text-align:center;">
64
</td>
</tr>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
52
</td>
<td style="text-align:center;">
-12
</td>
<td style="text-align:center;">
144
</td>
</tr>
<tr>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
20
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
-19
</td>
<td style="text-align:center;">
361
</td>
</tr>
</tbody>
</table>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-8">Table 5: </span>Observed values, predicted values, residuals, and squared residuals for Model B.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{Y_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
63
</td>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
13
</td>
<td style="text-align:center;">
169
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
14
</td>
<td style="text-align:center;">
196
</td>
</tr>
<tr>
<td style="text-align:center;">
30
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
-10
</td>
<td style="text-align:center;">
100
</td>
</tr>
<tr>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
70
</td>
<td style="text-align:center;">
-2
</td>
<td style="text-align:center;">
4
</td>
</tr>
<tr>
<td style="text-align:center;">
20
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
-15
</td>
<td style="text-align:center;">
225
</td>
</tr>
</tbody>
</table>
<p>Summing these squared values for each model we obtain:</p>
<ul>
<li><strong>Model A:</strong> SSE = 690</li>
<li><strong>Model B:</strong> SSE = 694</li>
</ul>
<p>Once we have quantified the model misfit, we can choose the model that has the least amount of error. Since Model A has a lower SSE than Model B, we would conclude that Model A is the better fitting model to the data.</p>
<p><br /></p>
<div id="visualizing-the-sse" class="section level3">
<h3>Visualizing the SSE</h3>
<p>To further understand the sum of squared error, we can examine a visual representation of the SSE for Model A. Recall that visually, the residual is the vertical distance between an observation and the fitted value (which lie on the fitted line). The residual indicates how different these two quantities are on the <em>Y</em>-metric. In the formula we squared each of the residuals. Visually, this is equivalent to producing the area of a square that has side length equal to the absolute value of the residual.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="05-ols-estimation_files/figure-html/unnamed-chunk-9-1.png" alt="Scatterplot of the observed toy data and the OLS fitted regression line for Model A. The left-hand plot visually displays the residual values as line segments with negative residuals shown as dashed lines. The right-hand plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual." width="45%" /><img src="05-ols-estimation_files/figure-html/unnamed-chunk-9-2.png" alt="Scatterplot of the observed toy data and the OLS fitted regression line for Model A. The left-hand plot visually displays the residual values as line segments with negative residuals shown as dashed lines. The right-hand plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual." width="45%" />
<p class="caption">
Figure 5: Scatterplot of the observed toy data and the OLS fitted regression line for Model A. The left-hand plot visually displays the residual values as line segments with negative residuals shown as dashed lines. The right-hand plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual.
</p>
</div>
<p>The SSE is simply the total area encompassed by all of the squares. Note that the observation that is directly on the line has a residual of 0 and thus does not contribute an quantity to the SSE. If you computed the SSE for a line with different intercept or slope values, the SSE will be different. The plot below shows what this might look like for the flat line produced by <span class="math inline">\(~~\hat{Y_i} = 50\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="05-ols-estimation_files/figure-html/unnamed-chunk-10-1.png" alt="Scatterplot of the observed toy data and the fitted flat line with Y-intercept of 50. The plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual." width="50%" />
<p class="caption">
Figure 10: Scatterplot of the observed toy data and the fitted flat line with Y-intercept of 50. The plot visually shows the squared residuals as the area of a square with side length equal to the absolute value of the residual.
</p>
</div>
<p><span class="citation">Powell &amp; Lehe (<a href="#ref-Powell:2015">2015</a>)</span> created an <a href="http://setosa.io/ev/ordinary-least-squares-regression/">interactive website</a> to help understand how the SSE is impacted by changing the intercept or slope of a line. You can also see how indicidual observations impact the SSE value.</p>
<p><br /></p>
</div>
</div>
<div id="best-fitting-model" class="section level2">
<h2>“Best” Fitting Model</h2>
<p>In the vocabulary of statistical estimation, the process we just used to adopt Model A over Model B was composed of two parts:</p>
<ul>
<li><strong>Quantification of Model Fit:</strong> We quantify how well (or not well) the estimated model fits the data; and</li>
<li><strong>Optimization:</strong> We find the “best” model based on that quantification. (This boils down to finding the model that produces the biggest or smallest measure of model fit.)</li>
</ul>
<p>In our example we used the SSE as the quantification of model fit, and then we optimized by selecting the model with the lower SSE. When we use <code>lm()</code> to fit a regression analysis to the data, R needs to consider not just two models like we did in our example, but all potential models (i.e., any intercept and slope). The model coefficeints that <code>lm()</code> returns are the “best” in that no other values for intercept or slope would produce a lower SSE. The model returned has the lowest SSE possible thus <em>least squares</em>. For our toy dataset, the model that produces the smallest residuals is</p>
<p><span class="math display">\[
\hat{Y}_i = 28.682 + 8.614(X_i)
\]</span></p>
<p>This model gives the following predicted values and residuals:</p>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-11">Table 6: </span>Observed values, predicted values, residuals, and squared residuals for the ‘best’ fitting model.
</caption>
<thead>
<tr>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{Y_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}\)</span>
</th>
<th style="text-align:center;text-align: center;">
<span class="math inline">\(\hat{\epsilon_i}^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
63
</td>
<td style="text-align:center;">
49.61364
</td>
<td style="text-align:center;">
13.386364
</td>
<td style="text-align:center;">
179.1947
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
44
</td>
<td style="text-align:center;">
33.47727
</td>
<td style="text-align:center;">
10.522727
</td>
<td style="text-align:center;">
110.7278
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
40
</td>
<td style="text-align:center;">
49.61364
</td>
<td style="text-align:center;">
-9.613636
</td>
<td style="text-align:center;">
92.4220
</td>
</tr>
<tr>
<td style="text-align:center;">
5
</td>
<td style="text-align:center;">
68
</td>
<td style="text-align:center;">
65.75000
</td>
<td style="text-align:center;">
2.250000
</td>
<td style="text-align:center;">
5.0625
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
25
</td>
<td style="text-align:center;">
41.54545
</td>
<td style="text-align:center;">
-16.545455
</td>
<td style="text-align:center;">
273.7521
</td>
</tr>
</tbody>
</table>
<p>The SSE is 661.16. This is the smallest SSE possible for a linear model. Any other value for the slope or intercept would result in a higher SSE.</p>
<p><br /></p>
<div id="mathematical-optimization" class="section level3">
<h3>Mathematical Optimization</h3>
<p>Finding the intercept and slope that give the lowest SSE is referred to as an optimization problem in the field of mathematics. Optimization is such an important (and sometimes difficult) probelm that there have been several mathematical and computational optimization methods that have been developed over the years. You can <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">read more about mathematical optimization on Wikipedia</a> if you are interested.</p>
<p>One common mathematical method to find the minimum SSE involves calculus. We would write the SSE as a function of<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, compute the partial derivatives (w.r.t. each of the coefficients), set these equal to zero, and solve to find the values of the coefficients. You can read <a href="https://isites.harvard.edu/fs/docs/icb.topic515975.files/OLSDerivation.pdf">here</a>. The <code>lm()</code> function actually uses an optimization method called <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> to obtain the regression coefficients. The actual mechanics and computation of these methods are beyond the scope of this course. We will just trust that the <code>lm()</code> function is doing things correctly in this course.</p>
<p><br /></p>
</div>
</div>
<div id="computing-the-sse-for-the-model-fitted-to-the-riverview-data" class="section level2">
<h2>Computing the SSE for the Model Fitted to the Riverview Data</h2>
<p>Since the regression model is based on the lowest SSE, it is often useful to compute and report the model’s SSE. We can use R to compute the SSE by carrying out the computations underlying the formula for SSE. Recall that the SSE is</p>
<p><span class="math display">\[
\mathrm{SSE} = \sum \left( Y_i - \hat{Y}_i\right)^2
\]</span></p>
<p>We need to compute the:</p>
<ol style="list-style-type: decimal">
<li>Predicted values (<span class="math inline">\(\hat{Y}_i\)</span>);</li>
<li>Residuals (<span class="math inline">\(e_i\)</span>);</li>
<li>Squared residuals (<span class="math inline">\(e_i^2\)</span>); and finally,</li>
<li>Sum of the squared residuals (<span class="math inline">\(\sum e_i^2\)</span>).</li>
</ol>
<p>From the Riverview data set we have the observed <span class="math inline">\(X\)</span> (education level) and <span class="math inline">\(Y\)</span> (income) values, and from the fitted <code>lm()</code> we have the intercept and slope estimates for the ‘best’ fitting regression model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" title="1"><span class="co"># Step 1: Compute the predicted values of Y</span></a>
<a class="sourceLine" id="cb22-2" title="2">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb22-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb22-4" title="4">    <span class="dt">y_hat =</span> <span class="fl">11.321</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.651</span> <span class="op">*</span><span class="st"> </span>education</a>
<a class="sourceLine" id="cb22-5" title="5">    )</a></code></pre></div>
<pre><code># A tibble: 32 x 7
   education income seniority gender  male party       y_hat
       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;
 1         8   37.4         7 male       1 Democrat     32.5
 2         8   26.4         9 female     0 Independent  32.5
 3        10   47.0        14 male       1 Democrat     37.8
 4        10   34.2        16 female     0 Independent  37.8
 5        10   25.5         1 female     0 Republican   37.8
 6        12   46.5        11 female     0 Democrat     43.1
 7        12   37.7        14 male       1 Democrat     43.1
 8        12   50.3        24 male       1 Democrat     43.1
 9        12   52.5        16 female     0 Independent  43.1
10        14   32.6         5 female     0 Independent  48.4
# … with 22 more rows</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" title="1"><span class="co"># Step 2: Compute the residuals</span></a>
<a class="sourceLine" id="cb24-2" title="2">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb24-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb24-4" title="4">    <span class="dt">y_hat =</span> <span class="fl">11.321</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.651</span> <span class="op">*</span><span class="st"> </span>education,</a>
<a class="sourceLine" id="cb24-5" title="5">    <span class="dt">errors =</span> income <span class="op">-</span><span class="st"> </span>y_hat</a>
<a class="sourceLine" id="cb24-6" title="6">    )</a></code></pre></div>
<pre><code># A tibble: 32 x 8
   education income seniority gender  male party       y_hat errors
       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;
 1         8   37.4         7 male       1 Democrat     32.5   4.92
 2         8   26.4         9 female     0 Independent  32.5  -6.10
 3        10   47.0        14 male       1 Democrat     37.8   9.20
 4        10   34.2        16 female     0 Independent  37.8  -3.65
 5        10   25.5         1 female     0 Republican   37.8 -12.4 
 6        12   46.5        11 female     0 Democrat     43.1   3.36
 7        12   37.7        14 male       1 Democrat     43.1  -5.48
 8        12   50.3        24 male       1 Democrat     43.1   7.13
 9        12   52.5        16 female     0 Independent  43.1   9.35
10        14   32.6         5 female     0 Independent  48.4 -15.8 
# … with 22 more rows</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" title="1"><span class="co"># Step 3: Compute the squared residuals</span></a>
<a class="sourceLine" id="cb26-2" title="2">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb26-4" title="4">    <span class="dt">y_hat =</span> <span class="fl">11.321</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.651</span> <span class="op">*</span><span class="st"> </span>education,</a>
<a class="sourceLine" id="cb26-5" title="5">    <span class="dt">errors =</span> income <span class="op">-</span><span class="st"> </span>y_hat,</a>
<a class="sourceLine" id="cb26-6" title="6">    <span class="dt">sq_errors =</span> errors <span class="op">^</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb26-7" title="7">  )</a></code></pre></div>
<pre><code># A tibble: 32 x 9
   education income seniority gender  male party       y_hat errors sq_errors
       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
 1         8   37.4         7 male       1 Democrat     32.5   4.92      24.2
 2         8   26.4         9 female     0 Independent  32.5  -6.10      37.2
 3        10   47.0        14 male       1 Democrat     37.8   9.20      84.7
 4        10   34.2        16 female     0 Independent  37.8  -3.65      13.3
 5        10   25.5         1 female     0 Republican   37.8 -12.4      153. 
 6        12   46.5        11 female     0 Democrat     43.1   3.36      11.3
 7        12   37.7        14 male       1 Democrat     43.1  -5.48      30.0
 8        12   50.3        24 male       1 Democrat     43.1   7.13      50.9
 9        12   52.5        16 female     0 Independent  43.1   9.35      87.4
10        14   32.6         5 female     0 Independent  48.4 -15.8      250. 
# … with 22 more rows</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" title="1"><span class="co"># Step 4: Compute the sum of the squared residuals</span></a>
<a class="sourceLine" id="cb28-2" title="2">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb28-4" title="4">    <span class="dt">y_hat =</span> <span class="fl">11.321</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.651</span> <span class="op">*</span><span class="st"> </span>education,</a>
<a class="sourceLine" id="cb28-5" title="5">    <span class="dt">errors =</span> income <span class="op">-</span><span class="st"> </span>y_hat,</a>
<a class="sourceLine" id="cb28-6" title="6">    <span class="dt">sq_errors =</span> errors <span class="op">^</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb28-7" title="7">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-8" title="8"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb28-9" title="9">    <span class="dt">SSE =</span> <span class="kw">sum</span>(sq_errors)</a>
<a class="sourceLine" id="cb28-10" title="10">  )</a></code></pre></div>
<pre><code># A tibble: 1 x 1
    SSE
  &lt;dbl&gt;
1 2418.</code></pre>
<p>The SSE gives us information about the variation in <span class="math inline">\(Y\)</span> (the outcome variable) that is left over (residual) after we fit the regression model. Since the regression model is a function of <span class="math inline">\(X\)</span>, the SSE tells us about the variation in <span class="math inline">\(Y\)</span> that is left over after we remove the variation associated with, or accounted for by <span class="math inline">\(X\)</span>. In our example it tells us about the residual variation in incomes after we account for employee education level.</p>
<p>In practice, we often report the SSE, but <em>we do not interpret the actual value</em>. The value of the SSE is more useful when comparing models. When researchers are considering different models, the SSEs from these models are compared to determine which model produces the least amount of misfit to the data (similar to what we did earlier).</p>
<p><br /></p>
<div id="evaluating-the-impact-of-a-predictor-using-sse" class="section level3">
<h3>Evaluating the Impact of a Predictor Using SSE</h3>
<p>Consider again the general equation for the statistical model that includes a single predictor,</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1(X_i) + \epsilon_i
\]</span></p>
<p>One way that statisticians evaluate a predictor is to compare a model that includes that predictor to the same model that does not include that predictor. For example, comparing the following two models allows us to evaluate the impact of <span class="math inline">\(X_i\)</span>.</p>
<p><span class="math display">\[
\begin{split}
Y_i &amp;= \beta_0 + \beta_1(X_i) + \epsilon_i \\
Y_i &amp;= \beta_0 + \epsilon_i
\end{split}
\]</span></p>
<p>The second model, without the effect of <span class="math inline">\(X\)</span>, is referred to as the <em>intercept-only model</em>. This model implies that the value of <span class="math inline">\(Y\)</span> is not a function of <span class="math inline">\(X\)</span>. In our example it suggests that the mean income is not conditional on education level. The fitted equation,</p>
<p><span class="math display">\[
\hat{Y}_i = \hat{\beta}_0
\]</span></p>
<p>indicates that the predicted <span class="math inline">\(Y\)</span> would be the same (constant) regardless of what <span class="math inline">\(X\)</span> is. In our example, this would be equivalent to saying that the mean income is the same, regardless ofemployee education level.</p>
<p>&lt; br /&gt;</p>
</div>
<div id="fitting-the-intercept-only-model" class="section level3">
<h3>Fitting the Intercept-Only Model</h3>
<p>To fit the intercept-only model, we just omit the prediter term on the right-hand side of the <code>lm()</code> formula.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" title="1">lm<span class="fl">.0</span> =<span class="st"> </span><span class="kw">lm</span>(income <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> city)</a>
<a class="sourceLine" id="cb30-2" title="2">lm<span class="fl">.0</span></a></code></pre></div>
<pre><code>
Call:
lm(formula = income ~ 1, data = city)

Coefficients:
(Intercept)  
      53.74  </code></pre>
<p>The fitted regression equation for the intercept-only model can be written as,</p>
<p><span class="math display">\[
\hat{\mathrm{Income}_i} = 53.742
\]</span></p>
<p>Graphically, the fitted line is a flat line crossing the <span class="math inline">\(y\)</span>-axis at 53.742 (see plot below).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="05-ols-estimation_files/figure-html/unnamed-chunk-14-1.png" alt="Scatterplot of employee incomes versus education levels. The OLS fitted regression line for the intercept-only model is also displayed." width="50%" />
<p class="caption">
Figure 11: Scatterplot of employee incomes versus education levels. The OLS fitted regression line for the intercept-only model is also displayed.
</p>
</div>
<p>Does the estimate for <span class="math inline">\(\beta_0\)</span>, 53.742, seem familiar? If not, go back to the exploration of the response variable in the <a href="#sldr">Simple Linear Regression—Description</a> chapter. The estimated intercept in the intercept-only model is the marginal mean value of the response variable. This is not a coincidence.</p>
<p>Remember that the regression model estimates the mean. Here, since the model is not a conditional model (no <span class="math inline">\(X\)</span> predictor) the expected value (mean) is the marginal mean.</p>
<p><span class="math display">\[
\begin{split}
\mu_Y &amp;= \beta_0 \\
\end{split}
\]</span></p>
<p>Plotting this we get,</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="05-ols-estimation_files/figure-html/unnamed-chunk-15-1.png" alt="Plot displaying the OLS fitted regression line for the intercept-only model. Histogram showing the marginal distributon of incomes is also shown." width="50%" />
<p class="caption">
Figure 12: Plot displaying the OLS fitted regression line for the intercept-only model. Histogram showing the marginal distributon of incomes is also shown.
</p>
</div>
<p>The model itself does not consider any predictors, so on the plot, the <span class="math inline">\(X\)</span> variable is superfluous; we could just collapse it to its margin. This is why the mean of all the <span class="math inline">\(Y\)</span> values is sometimes referred to as the <em>marginal mean</em>.</p>
<p>Yet another way to think about this is that the model is choosing a single income (<span class="math inline">\(\hat{\beta}_0\)</span>) to be the predicted income for all the employees. Which value would be a good choice? Remember the <code>lm()</code> function chooses the “best” value for the parameter estimate based on minimizing the sum of squared errors. The marginal mean is the value that minimizes the squared deviations (errors) across all of the observations, regardless of education level. This is one reason the mean is often used as a summary measure of a set of data.</p>
<p><br /></p>
</div>
<div id="sse-for-the-intercept-only-model" class="section level3">
<h3>SSE for the Intercept-Only Model</h3>
<p>Since the intercept-only model does not include any predictors, the SSE for this model is a quantification of the total variation in the outcome variable. It can be used as baseline measure of the error variation in the data. Below we compute the SSE for the intercept-only model (if you need to go through the steps one-at-a-time, do so.)</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" title="1">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb32-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb32-3" title="3">    <span class="dt">y_hat =</span> <span class="fl">53.742</span>,</a>
<a class="sourceLine" id="cb32-4" title="4">    <span class="dt">errors =</span> income <span class="op">-</span><span class="st"> </span>y_hat,</a>
<a class="sourceLine" id="cb32-5" title="5">    <span class="dt">sq_errors =</span> errors <span class="op">^</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb32-6" title="6">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb32-7" title="7"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb32-8" title="8">    <span class="dt">SSE =</span> <span class="kw">sum</span>(sq_errors)</a>
<a class="sourceLine" id="cb32-9" title="9">  )</a></code></pre></div>
<pre><code># A tibble: 1 x 1
    SSE
  &lt;dbl&gt;
1 6566.</code></pre>
<p><br /></p>
</div>
<div id="proportion-reduction-in-error" class="section level3">
<h3>Proportion Reduction in Error</h3>
<p>The SSE for the intercept-only model represents the total amount of variation in the sample incomes. As such we can use it as a baseline for comparing other models that include predictors. For example,</p>
<ul>
<li><strong>SSE (Intercept-Only):</strong> 6566</li>
<li><strong>SSE (w/Education Level Predictor):</strong> 2418</li>
</ul>
<p>Once we account for education in the model, we reduce the SSE. Moreover, since the only difference between the intercept-only model and the preictor model was the inclusion of the effect of education level, any difference in the SSE is attributable to including education in the model. Since the SSE is smaller after we include education level in the model it implies that improving the data–model fit (smaller error).</p>
<p>How much did the amount of error improve? The SSE was reduced by 4148 after including education level in the model. Is this a lot? To answer that question, we typically compute and report this reduction as a proportion of the total variation; called the <em>proportion of the reduction in error</em>, or PRE.</p>
<p><span class="math display">\[
\mathrm{PRE} = \frac{\mathrm{SSE}_{\mathrm{Intercept\mbox{-}Only}} - \mathrm{SSE}_{\mathrm{Predictor\mbox{-}Model}}}{\mathrm{SSE}_{\mathrm{Intercept\mbox{-}Only}}}
\]</span></p>
<p>For our particular example,</p>
<p><span class="math display">\[
\begin{split}
\mathrm{PRE} &amp;= \frac{6566 - 2418}{6566} \\
&amp;= \frac{4148}{6566} \\
&amp;= 0.632
\end{split}
\]</span></p>
<p>Including education level as a predictor in the model reduced the error by 63.2%.</p>
<p><br /></p>
</div>
</div>
<div id="partitioning-variation" class="section level2">
<h2>Partitioning Variation</h2>
<p>Using the SSE terms we can partition the total variation in <span class="math inline">\(Y\)</span> (the SSE value from the intercept-only model) into two parts: (1) the part that is explained by the model, and (2) the part that remains unexplained. The unexplained variation is just the SSE from the regression model that includes <span class="math inline">\(X\)</span>; remember it is <em>residual variation</em>. Here is the partitioning of the variation in income.</p>
<p><span class="math display">\[
\underbrace{6566}_{\substack{\text{Total} \\ \text{Variation}}} = \underbrace{4148}_{\substack{\text{Explained} \\ \text{Variation}}} + \underbrace{2418}_{\substack{\text{Unexplained} \\ \text{Variation}}}
\]</span></p>
<p>Each of these three terms is a sum of squares (SS). The first is referred to as the <em>total sum of squares</em>, as it represents the total amount of variation in <span class="math inline">\(Y\)</span>. The second term is commmonly called the <em>model sum of squares</em> (or, regression sum of squares), as it represents the variation explained by the model. The last term is the <em>error sum of squares</em> (or, residual sum of squares) as it represents the left-over variation that is unexplained by the model.</p>
<p>More generally,</p>
<p><span class="math display">\[
\mathrm{SS_{\mathrm{Total}}} = \mathrm{SS_{\mathrm{Model}}} + \mathrm{SS_{\mathrm{Error}}}
\]</span></p>
<div id="variation-accounted-for" class="section level3">
<h3>Variation Accounted For</h3>
<p>It is often convienient to express these values as proportions of the total variation. To do this we can divide each term in the partitioning by the totl sum of squares.</p>
<p><span class="math display">\[
\frac{\mathrm{SS_{\mathrm{Total}}}}{\mathrm{SS_{\mathrm{Total}}}} = \frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}} + \frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}}
\]</span></p>
<p>Using the values from our example,</p>
<p><span class="math display">\[
\begin{split}
\frac{6566}{6566} &amp;= \frac{4148}{6566} + \frac{2418}{6566} \\[1ex]
1 &amp;= 0.632 + 0.368
\end{split}
\]</span></p>
<p>The first term on the right-hand side of the equation, <span class="math inline">\(\frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}}\)</span>, is 0.632. This is the PRE value we computed earlier. Since the <span class="math inline">\(\mathrm{SS_{\mathrm{Model}}}\)</span> represents the model-explained variation, many researchers interpret this value as the percentage of <em>variation explained</em> or accounted for by the model. They might say,</p>
<blockquote>
<p>The model accounts for 63.2% of the variation in incomes.</p>
</blockquote>
<p>Since the only predictor in the model is education level, an alternative interpretation of this value is,</p>
<blockquote>
<p>Differences in education level account for 63.2% of the variation in incomes.</p>
</blockquote>
<p>Better models explain more variation in the outcome. They also have small errors. Aside from conceptually making some sense, this is also shown in the mathematics of the partitioning of variation.</p>
<p><span class="math display">\[
1 = \frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}} + \frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}}
\]</span></p>
<p>Since the denominator is the same on both terms, and the sum of the two terms must be one, this implies that the smaller the amount of error, the smaller the last term (proportion of unexplained variation ) must be and the larger the first term (the proportion of explained variation) has to be.</p>
<p><br /></p>
</div>
<div id="r-squared" class="section level3">
<h3>R-Squared</h3>
<p>Another way to think about measuring the quality of a model is that ‘good’ models should reproduce the observed outcomes, after all they explain variation in the outcome. How well do the fitted (predicted) values from our model match wih the outcome values? To find out, we can compute the correlation between the model fitted values and the observed outcome values. To compute a correlation, we will use the <code>correlate()</code> function from the **corrr* package.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" title="1"><span class="co"># Load library</span></a>
<a class="sourceLine" id="cb34-2" title="2"><span class="kw">library</span>(corrr)</a>
<a class="sourceLine" id="cb34-3" title="3"></a>
<a class="sourceLine" id="cb34-4" title="4"><span class="co"># Create fitted values and correlate them with the outcome</span></a>
<a class="sourceLine" id="cb34-5" title="5">city <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb34-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb34-7" title="7">    <span class="dt">y_hat =</span> <span class="fl">11.321</span> <span class="op">+</span><span class="st"> </span><span class="fl">2.651</span><span class="op">*</span>education</a>
<a class="sourceLine" id="cb34-8" title="8">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb34-9" title="9"><span class="st">  </span><span class="kw">select</span>(y_hat, income) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb34-10" title="10"><span class="st">  </span><span class="kw">correlate</span>()</a></code></pre></div>
<pre><code># A tibble: 2 x 3
  rowname  y_hat income
  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
1 y_hat   NA      0.795
2 income   0.795 NA    </code></pre>
<p>The correlation between the observed and fitted values is 0.795. This is a high correlation indicating that the model fitted values and the observed values are similar. We denote this value using the uppercase Roman letter <span class="math inline">\(R\)</span>.</p>
<p><span class="math display">\[
R = 0.795
\]</span></p>
<p>Now square this value.</p>
<p><span class="math display">\[
R^2 = 0.795^2 = 0.632
\]</span></p>
<p>Again we get the PRE value! All three ways of expressing this metric of model quality are equivalent:</p>
<ul>
<li><span class="math inline">\(\frac{\mathrm{SSE}_{\mathrm{Intercept\mbox{-}Only}} - \mathrm{SSE}_{\mathrm{Predictor\mbox{-}Model}}}{\mathrm{SSE}_{\mathrm{Intercept\mbox{-}Only}}}\)</span></li>
<li><span class="math inline">\(\frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}}\)</span></li>
<li><span class="math inline">\(R^2\)</span></li>
</ul>
<p>Although these indices seem to measure different aspects of model quality—reduction in error variation, model explained variation, and alignment of the model fitted and observed values—with OLS fitted linear models, these values are all equal. This will not necessarily be true when we estimate model parameters using a different estimation method (e.g., maximum likelihood). Most of the time this value will be reported in applied research as <span class="math inline">\(R^2\)</span>, but as you can see, there are many interpretations of this value under the OLS framework.</p>
<p><br /></p>
</div>
<div id="back-to-partitioning" class="section level3">
<h3>Back to Partitioning</h3>
<p>Using the fact that <span class="math inline">\(R^2 = \frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}}\)</span>, we can substitute this into the partitioning equation from earlier.</p>
<p><span class="math display">\[
\begin{split}
\frac{\mathrm{SS_{\mathrm{Total}}}}{\mathrm{SS_{\mathrm{Total}}}} &amp;= \frac{\mathrm{SS_{\mathrm{Model}}}}{\mathrm{SS_{\mathrm{Total}}}} + \frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}} \\[1ex]
1 &amp;= R^2 + \frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}} \\[1ex]
1 - R^2 &amp;= \frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}} 
\end{split}
\]</span></p>
<p>This suggests that the last term in the partititoning, <span class="math inline">\(\frac{\mathrm{SS_{\mathrm{Error}}}}{\mathrm{SS_{\mathrm{Total}}}}\)</span> is simply the difference between 1 and <span class="math inline">\(R^2\)</span>. In our example,</p>
<ul>
<li><span class="math inline">\(R^2 = 0.632\)</span>, and</li>
<li><span class="math inline">\(1 - R^2 = 0.368\)</span></li>
</ul>
<p>Remember that one interpretation of <span class="math inline">\(R^2\)</span> is that 63.2% of the variation in incomes was explained by the model. Alternatively, 36.8% of the variation in income is not explained by the model; it is residual variation. If the unexplained variation is too large, it suggests to an applied analyst that she could include additional predictors in the model. We will explore this in future chapters.</p>
<p>For now, recognize that OLS estimation gives us the ‘best’ model in terms of minimizing the sum of squared residuals, which in turn maximizes the explained variation. But, the ‘best’ model may not be a ‘good’ model. One way to measure quality of the model is through the metric of <span class="math inline">\(R^2\)</span>. Understanding whether <span class="math inline">\(R^2\)</span> is large or small is based on the domain science. For example in some areas of educational research, an <span class="math inline">\(R^2\)</span> of 0.4 might indicate a really great model, whereas the same <span class="math inline">\(R^2\)</span> of 0.4 in some areas of biological research might be quite small and indicate a poor model.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Powell:2015">
<p>Powell, V., &amp; Lehe, L. (2015). <em>Ordicary least squares regression: Explained visually</em>. Explained Visually: A Setosa Project. <a href="https://setosa.io/ev/ordinary-least-squares-regression/">https://setosa.io/ev/ordinary-least-squares-regression/</a></p>
</div>
</div>
<p style="text-align: center;">
<a href="slrd.html"><button class="btn btn-default">Previous</button></a>
<a href="cor.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>

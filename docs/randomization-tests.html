<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Randomization and Permutation Tests | Comparing Groups (Draft)" />
<meta property="og:type" content="book" />


<meta property="og:description" content="A draft of an updated version of Comparing Groups" />
<meta name="github-repo" content="zief0002/comparing-groups-draft" />

<meta name="author" content="Andrew Zieffler, Jeffrey Harring, and Jeffrey Long" />

<meta name="date" content="2020-04-12" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A draft of an updated version of Comparing Groups">

<title>Randomization and Permutation Tests | Comparing Groups (Draft)</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="style/style.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/navbar.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#foreword-second-edition">Foreword (Second Edition)</a></li>
<li><a href="foreword-first-edition.html#foreword-first-edition">Foreword (First Edition)</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="KDE.html#KDE">Kernel Density Estimation</a></li>
<li><a href="exploration.html#exploration">Exploration: Comparing Two Groups</a></li>
<li><a href="multivariate-exploration.html#multivariate-exploration">Exploration: Comparing Many Groups</a></li>
<li><a href="randomization-tests.html#randomization-tests">Randomization and Permutation Tests</a></li>
<li><a href="final-words.html#final-words">Final Words</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="randomization-tests" class="section level1">
<h1>Randomization and Permutation Tests</h1>
<p>Adapted from <span class="citation">Zieffler et al. (<a href="#ref-Zieffler:2011">2011</a>)</span></p>
<p><br /><br /></p>
<div class="note">
<p>This chapter assumes a working knowledge of <strong>dplyr</strong> and <strong>ggplot2</strong> functionality to work with and plot data.</p>
</div>
<p><br /><br /></p>
<p>In the chapter <a href="exploration.html#exploration">Exploration: Comparing Two Groups</a>, differences between two groups were examined. Specifically, the question of whether there were differences in the annual household per capita expenditures between the rural and urban populations in Vietnam was addressed. In that chapter, exploratory methods, such as graphical and numerical summarizations, were used to quantify the differences in the two distributions of household per capita expenditures. Exploration is often only the starting point for examining research questions involving group differences.</p>
<p>These methods do not always provide a complete answer to the research question. For example, most educational and behavioral researchers also want to determine whether the differences that might have shown up in the exploration phase are “real,” and to what population(s) the “real” effect can be attributed. A “real” effect is a sample effect that is caused by an actual difference in the population of interest. For example, suppose the mean per capita household expenditures for the entirety of Vietnam is actually less for rural regions. Then a sample result would be expected to reflect this, provided the sample was obtained in a particular way, namely, randomly (see below). In addition to evaluating whether effects are “real”, it is important to estimate the size of the effect. Uncertainty is always involved in this endeavor, which relates to the <em>precision</em> of the estimate.</p>
<p>Questions of whether or not group differences are “real”, estimates of the size of group differences, and the precision of these estimates are typically problems of <em>statistical inference</em>. First, however, two research questions regarding group differences that have been studied by educational and behavioral scientists are presented.</p>
<p><br /><br /></p>
<div id="research-question-1" class="section level3">
<h3>Research Question 1</h3>
<p>Demands for accountability and delinquency prevention in recent years have led to rising popularity of after-school programs in the United States. The intuitive appeal of these programs is based on the perception that adolescents left unsupervised will either simply waste time or, worse, engage in delinquent and dangerous behaviors.</p>
<p>To empirically study the effects of attending an after-school program, <span class="citation">Gottfredson et al. (<a href="#ref-gottfredson">2010</a>)</span> randomly assigned middle-school students to either a treatment group or control group. The treatment consisted of participation in an after-school program, whereas the control group engaged in their usual routine, but control students were invited to attend one after-school activity per month. Data on several outcome measures were collected on the study participants. These data are available in <a href="https://raw.githubusercontent.com/zief0002/musings/master/data/after-school.csv">after-school.csv</a>. The researchers were interested in determining whether there is a difference in the effect of delinquency between students who participated in the after-school program and students that did not.</p>
<p><br /><br /></p>
</div>
<div id="research-question-2" class="section level3">
<h3>Research Question 2</h3>
<p>The Center for Immigration Studies at the United States Census Bureau has reported that despite shifts in the ethnic makeup of the immigrant population, Latin America—and Mexico specifically—remains this country’s greatest source of immigrants. Although the average immigrant is approximately 40 years old, large numbers are children who enroll in U.S. schools upon arrival. Their subsequent educational achievement affects not only their own economic prospects but also those of their families, communities, and the nation as a whole.</p>
<p><span class="citation">Stamps &amp; Bohon (<a href="#ref-stamps">2006</a>)</span> studied the educational achievement of Latino immigrants by examining a random sample of the 2000 decennial Census data, a subset of which is provided in <a href="https://raw.githubusercontent.com/zief0002/musings/master/data/latino-education.csv">latino-education.csv</a>. One interesting research question that has emerged from their research is whether there is a link between where the immigrants originated and their subsequent educational achievement. Specifically, the question is if there is a difference in the educational achievement of immigrants from Mexico and that of immigrants from other Latin American countries.</p>
<p><br /><br /></p>
</div>
<div id="random-assignment-random-sampling" class="section level3">
<h3>Random Assignment &amp; Random Sampling</h3>
<p>While both of these research questions may seem similar—apart from their context—they are in fact very different. In the first situation, the researchers used a volunteer sample and randomly assigned the participants in their sample to the two groups—treatment and control. In the second situation, the researchers randomly selected their sample from a larger population (the 2000 census), but the two groups were not assigned by the researchers. These examples illustrate two important differences: (1) how the sample is selected and (2) how the treatments, or groups, are assigned. Table <a href="exploration.html#tab:tab-01">1</a> shows the four potential situations that educational and behavioral science researchers could face.</p>
<table style="width:70%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:tab-01">Table 1: </span>Four Potential Scenarios Researcher Could Face When Making Inferences
</caption>
<thead>
<tr>
<th style="text-align:left;text-align: center;">
Scenario
</th>
<th style="text-align:center;text-align: center;">
RS<span class="math inline">\(^a\)</span>
</th>
<th style="text-align:center;text-align: center;">
RA<span class="math inline">\(^b\)</span>
</th>
<th style="text-align:left;text-align: center;">
Type of Research
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Scenario 1
</td>
<td style="text-align:center;">
✓
</td>
<td style="text-align:center;">
</td>
<td style="text-align:left;">
Generalizable research
</td>
</tr>
<tr>
<td style="text-align:left;">
Scenario 2
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
✓
</td>
<td style="text-align:left;">
Randomized, experimental research
</td>
</tr>
<tr>
<td style="text-align:left;">
Scenario 3
</td>
<td style="text-align:center;">
✓
</td>
<td style="text-align:center;">
✓
</td>
<td style="text-align:left;">
Generalizable, randomized, experimental research
</td>
</tr>
<tr>
<td style="text-align:left;">
Scenario 4
</td>
<td style="text-align:center;">
</td>
<td style="text-align:center;">
</td>
<td style="text-align:left;">
Nongeneralizable, nonexperimental research
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup>a</sup> RS = Random sample
</td>
</tr>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup>b</sup> RA = Random assignment
</td>
</tr>
</tfoot>
</table>
<p>Each of these scenarios impacts the conclusions that a researcher can draw from quantitative results. How the sample is selected has a direct impact on the generalizations that a researcher can draw. For example, random sampling helps ensure that the conclusions drawn from the sample data can be generalized to the population from which the sample was drawn. In contrast, how the treatments are assigned has a direct impact on the causal inferences a researcher can make. Random assignment to treatments facilitate these causal inferences by allowing the attribution of sample differences to the differences in treatments.</p>
<p>The nomenclature used in Table <a href="exploration.html#tab:tab-01">1</a> is employed in this monograph to help educational and behavioral researchers make clearer distinctions between these scenarios. Unfortunately, the discipline of statistics uses no consistent terminology to describe each of these scenarios. For example, in research that employ random assignment, the term “experiment” is sometimes used, but as <span class="citation">Kempthorne (<a href="#ref-kempthorne4">1979</a>, p. 124)</span> points out:</p>
<p class="actualquote">
The literature of statistics has been plagued with difficulties about the use of the word “experiment.” Interestingly enough, so also has the general world of science. The problem is that a loose use of the word “experiment” permits it to be applied to any process of examination of a space-time section of the world.
</p>
<p>The use of random sampling and/or random assignment are the components that allow statistical inference to take place. The mathematical theory for inferential methods is, in fact, intrinsically tied to the employment of one or both of these random mechanisms. In this chapter, methods that allow researchers to answer research questions in which the researcher has randomly assigned the treatments are examined. In the chapter <a href="#bootstrp-tests">Bootstrap Tests</a>, methods that allow researchers to answer research questions if the study used random sampling to select the sample are examined. The other two scenarios—generalizable, randomized experimental research and nongeneralizable research—are touched on in this chapter and the <a href="#bootstrp-tests">Bootstrap Tests</a> chapters and are discussed further in the <a href="#philosophical-considerations">Philosophical Considerations</a> chapter.</p>
<p><br /><br /></p>
</div>
<div id="randomized-experimental-research" class="section level2">
<h2>Randomized Experimental Research</h2>
<p>Consider researchers who are studying whether after-school programs have an effect on delinquency. What if they observed that students who participate in after-school programs tended to have a low measure of delinquency? What could be said about the effect of after-school programs on delinquency? Should the researchers conclude that after-school programs lessen delinquency for students? That, of course, is one explanation for the observed relationship. However, there are several alternative explanations as well. One rival explanation is that students with a low propensity for delinquency to begin with were the ones who participated in the after-school program. And, had these students not participated in the after-school program, they still would have had low measures of delinquency. Because there is no comparison group—no variation in the treatment predictor—there is no way to establish which explanation is correct.</p>
<p>When examining the effect of a treatment or intervention, it is essential that educational and behavioral researchers specify a comparison group—often referred to as a <em>control group</em>. The comparison group attempts to answer the question, what would have happened to the group of students if they did not receive the treatment? Without a comparison group, it is impossible to rule out alternative explanations for the “effect” that is being examined. In fact, because the conclusions drawn from research that is conducted without comparison groups are relatively weak, many experts suggest that such research is a waste of time and “at best, offer indecisive answers to the research questions, and, at worst, might lead to erroneous conclusions” <span class="citation">(Light et al., <a href="#ref-light">1990</a>, p. 104–105)</span>."</p>
<p>What comparison group should the after-school program researchers choose? Perhaps they should compare the students who participated in the after-school program to other students who did not participate in the program. Should the students in the comparison group be from the same school as those in the treatment group? Or maybe from the same neighborhoods? Another option is to use the same students in the comparison and treatment group. The researchers could compare these students’ delinquency measures both before and after they participated in the after-school program. This type of design is often referred to as a <em>pre–post design</em> and will be examined in more detail in the <a href="#dependent-samples">Dependent Samples</a> chapter. Each of these comparison groups would yield a different answer to whether or not the after-school program has an effect on delinquency. Furthermore, depending on how the comparison group is selected, there may still be alternative explanations that cannot be ruled out, because the apparent effects could be due to attributes or characteristics, called <em>confounding variables</em>, that are systematically related to the treatment (e.g., socioeconomic status, scholastic engagement, etc.).</p>
<p>Because of the potential problems with confounding variables, the choice of a comparison group is an important task in research design. The best comparison group is one that is “composed of people who are similar to the people in the treatment group in all ways except that they did not receive the treatment” <span class="citation">(Light et al., <a href="#ref-light">1990</a>, p. 106)</span>. The use of random assignment, or randomization, to assign students to <em>conditions</em> is a statistical means of simultaneously considering all of the potential confounding variables, both known and unknown. The word <em>condition</em> is a generic term that includes the comparison group and the treatment group. Specifically, random assigment of participants to conditions (or conditions to participants) is a method that ensures that the treatment group and the comparison group are equivalent, <em>on average</em>, for all attributes, characteristics, and variables other than the treatment.</p>
<p>Again, consider the after-school program research study described in the previous section. Because the researchers randomly assigned middle-school students to either a treatment group or control group, the two groups of students should be equivalent, on average, for any variable that might be related to the treatment. Because of the randomization, any effects or differences in delinquency that the researchers find, must be due to the variation in conditions because the equivalence induced by the random assignment rules out other explanations. Variation in conditions in this context means the students either participated in an after-school program or they did not.</p>
<p><br /><br /></p>
</div>
<div id="introduction-to-the-randomization-test" class="section level2">
<h2>Introduction to the Randomization Test</h2>
<p>Randomization tests are statistical tests that can be used to evaluate hypotheses about treatment effects when experimental units have been randomly assigned to treatment conditions. To help illustrate the concept of the randomization tests, a pedagogical example is considered. Imagine a counseling psychologist was interested in determining if there was an effect of cognitive-behavioral and social problem-solving training on perceived social competence in school-aged aggressive boys.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> The researcher randomly assigned three aggressive boys to a control group and two others to a treatment group who received both cognitive-behavioral and social problem-solving training. After the study, a scale of perceived social competence was administered to all five participants, and their scores, respectively, were:</p>
<center>
<p>
<span>Treatment (T): 54, 66</span><span style="padding-left:70px;">Control (C): 57, 72, 30</span>
</p>
</center>
<p>with higher scores indicating a higher degree of perceived social competence. Does the higher average measure of perceived social competence in the treatment group—60 versus 53—provide convincing evidence that the training is effective? Is it possible that there is no effect of training, and that the difference observed could have arisen just from the nature of randomly assigning the five participants into groups? After all, it cannot be expected that randomization will always create perfectly equal groups. But, is it reasonable to believe the random assignment alone could have lead to this large of a difference? Or is the training also contributing to this difference?</p>
<p>To examine the above questions, one approach is to imagine the scenario under which the training had no effect whatsoever. One can go even further and always consider this scenario to be the default scenario. The purpose of statistical inference is to evaluate the default scenario. If there is sufficient evidence against the default scenario, then it would be discarded as implausible. If there is insufficient evidence against the default scenario, then it would not be discarded. Specifically, in this example, an assumption would be made that the training has no effect. Then, evidence would be collected to determine if the difference that was observed in the data is too large to probabilistically believe that there is no effect of training. This statement or assumption of no treatment effect is called the <em>null hypothesis</em> and is written as</p>
<center>
<p>
<span class="math inline">\(H_0:\)</span> The training is not effective.
</p>
</center>
<p>If the training is truly ineffective, then each participant’s perceived social competence score is only a function of that person and not a function of anything systematic, such as the training. The implication of the participant’s perceived social competence score not being a function of anything systematic is that, had a participant been assigned to the other condition (through a different random assignment), his perceived social competence score would have been identical since, in a sense, both conditions are doing nothing in terms of affecting the perceived social competence scores.</p>
<p>One can take advantage of the fact that each participant’s perceived social competence score would be identical whether they are assigned to treatment or control and examine <em>all possible</em> random assignments of the participants to conditions. Table <a href="exploration.html#tab:tab-02">2</a> shows all 10 possible permutations (i.e., arrangements) of the data, as well as the mean difference in perceived social competence scores for those assignments.</p>
<p>The notation <span class="math inline">\(\bar{T}\)</span> and <span class="math inline">\(\bar{C}\)</span> are used for the mean of the treatment group and mean of the control group, respectively. The term <em>permutation</em> here refers to a unique rearrangement of the data that rises from random assignment.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> Mathematically, there are</p>
<p><span class="math display">\[
{{5}\choose{2}} = \frac{5!}{2!(5-2)!} = 10,
\]</span></p>
<p>such rearrangements, where <span class="math inline">\(5! = 5 \times 4 \times 3 \times 2 \times 1\)</span>, and similarly for the other values. In general, the number of unique permutations of <span class="math inline">\(n\)</span> measurements into samples of size <span class="math inline">\(k\)</span> and <span class="math inline">\(n-k\)</span> is computed using</p>
<p><span class="math display">\[
{{n}\choose{k}} = \frac{n!}{k!(n-k)!},
\]</span></p>
<p>where <span class="math inline">\(n! = n \times (n-1) \times (n-2) \ldots \times 1\)</span>.</p>
<table style="width:80%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:tab-02">Table 2: </span>Ten Possible Permutations of the Perceived Social Competence Scores and Difference in Mean Perceived Social Competence Scores
</caption>
<thead>
<tr>
<th style="text-align:center;">
Permutation
</th>
<th style="text-align:center;">
Treatment
</th>
<th style="text-align:center;">
Control
</th>
<th style="text-align:left;">
<span class="math inline">\(\bar{T}-\bar{C}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
54, 66
</td>
<td style="text-align:center;">
57, 72, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(60-53=7\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
54, 57
</td>
<td style="text-align:center;">
66, 72, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(55.5-56=-0.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
54, 72
</td>
<td style="text-align:center;">
66, 57, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(63-51=12\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
54, 30
</td>
<td style="text-align:center;">
66, 72, 57
</td>
<td style="text-align:left;">
<span class="math inline">\(42-65=-23\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
5
</td>
<td style="text-align:center;">
57, 66
</td>
<td style="text-align:center;">
54, 72, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(61.5-52=9.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
6
</td>
<td style="text-align:center;">
72, 66
</td>
<td style="text-align:center;">
57, 54, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(69-47=22\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
7
</td>
<td style="text-align:center;">
30, 66
</td>
<td style="text-align:center;">
57, 72, 54
</td>
<td style="text-align:left;">
<span class="math inline">\(48-61=-13\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
8
</td>
<td style="text-align:center;">
57, 72
</td>
<td style="text-align:center;">
54, 66, 30
</td>
<td style="text-align:left;">
<span class="math inline">\(64.5-50=14.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
9
</td>
<td style="text-align:center;">
57, 30
</td>
<td style="text-align:center;">
66, 72, 54
</td>
<td style="text-align:left;">
<span class="math inline">\(43.5-64=-20.5\)</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
72, 30
</td>
<td style="text-align:center;">
66, 54, 57
</td>
<td style="text-align:left;">
<span class="math inline">\(51-59=-8\)</span>
</td>
</tr>
</tbody>
</table>
<p>The result that was observed, a difference of 7 points, is of course one of the possibilities under the assumption that there is no effect of training. The big question is whether or not it is likely that an observed mean difference of 7 points is large enough to say that it is due to something (i.e., the training) affecting the perceived social competence scores, or is it simply an artifact of the random assignment? Typically, educational and behavioral researchers provide a quantification of the strength of evidence against the null hypothesis called the <em>p</em>-value, which helps them answer this question. To compute the <em>p</em>-value, the proportion of random permutations of the data that provide a result <em>as extreme or more extreme</em> than the one observed is computed. Nine of the 10 potential results are as extreme or more extreme than a 7-point difference. Mathematically, this is written as</p>
<p><span class="math display">\[
P\biggl(\bigl|\textrm{observed \ difference}\bigr| \geq 7\biggr) = \frac{9}{10},
\]</span></p>
<p>where <span class="math inline">\(|\cdot|\)</span> is the absolute value.</p>
<p>If there is no effect of training, 9 out of the 10 permutations of the data that are possible would produce a result as extreme or more extreme than a 7-point difference. The <em>p</em>-value obtained from a study is a piece of evidence that can be used to evaluate the tenability of the null hypothesis. Smaller <em>p</em>-values provide stronger evidence against the null hypothesis. For example, the <em>p</em>-value of 0.5 provides very weak evidence against the null hypothesis that training is ineffective.</p>
<p>Many educational and behavioral researchers use a <em>p</em>-value to make a decision about the null hypothesis. This is a very bad idea. For one thing, <em>p</em>-values are tremendously impacted by the size of the sample. For example, in this example, the psychologist who was evaluating whether or not the training was effective had conditions in which the sample sizes were <span class="math inline">\(n_1=2\)</span> and <span class="math inline">\(n_2=3\)</span>. The <em>p</em>-value could have been large simply because the sample sizes were so small. With such little data, the number of possible permutations is limited, meaning the amount of information for evaluating the null hypothesis is also limited.</p>
<p>Secondly, decisions about null hypotheses often impact lines of research, and it is unclear if the results from one study should carry such weight. The null hypothesis test was developed by R. A. Fisher, who initially proposed the <em>p</em>-value as an informal index to be used as a measure of discrepancy between the observed data and the null hypothesis being tested, rather than a part of a formal inferential decision-making process <span class="citation">(Fisher, <a href="#ref-fisher4">1925</a>)</span>. He went on to suggest that <em>p</em>-values be used as part of the fluid, nonquantifiable process of drawing conclusions from observations, a process that included combining the <em>p</em>-value in no one specific manner with field substantive information and other research evidence. Above all, one can say that Fisher’s emphasis was on <em>replication</em> of results. Only if a result was replicated could it possible carry sufficient weight to impact a line of research.</p>
<p>This was best summed up by <span class="citation">Wasserstein &amp; Schirm (<a href="#ref-Wasserstein:2019">2019</a>)</span>, who gave the following advice about interpreting <em>p</em>-values from a study:</p>
<p class="actualquote">
Small <em>p</em>-values are like a right-swipe in Tinder. It means you have an interest. It doesn’t mean you’re ready to book the wedding venue.
</p>
<p>The methodology considered above is one example of a randomization test. As the plural, randomization tests, in the chapter title implies, this name refers to a class of procedures, and not a single test or procedure. All randomization tests, however, are derived from the same fundamental principle. Namely, that the reference distribution of a particular statistic can be obtained by assuming no differences between the groups. Under this assumption, the statistic of interest, such as the mean difference, is calculated under all the potential random assignments of the treatment conditions on the observed data. Then the reference distribution is used to evaluate the likelihood of seeing a result as extreme or more extreme than the one observed in the original data.</p>
<p><br /><br /></p>
</div>
<div id="randomization-tests-with-large-samples-monte-carlo-simulation" class="section level2">
<h2>Randomization Tests with Large Samples: Monte Carlo Simulation</h2>
<p>In the last section, the <em>p</em>-value calculated from the randomization test was <em>exact</em>. It was exact because <em>all possible</em> permutations of the data were used in the calculation of its value. When <span class="math inline">\(N=5\)</span>, it is possible to list all of the permutations of the data. However, for larger sample sizes, the computation of all the permutations becomes an impractical or even impossible task.</p>
<p>To illustrate, the data from the after-school research study introduced in Research Question 1 (see above) is examined to determine whether there is a difference in the effect of delinquency between students who participated in an after-school program and students that did not. These data can be found in <a href="https://raw.githubusercontent.com/zief0002/musings/master/data/after-school.csv">after-school.csv</a>. After an initial inspection of the data codebook, the data are imported into R and both graphical and numerical summaries can be examined (syntax below).</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" title="1"><span class="co"># Load libraries</span></a>
<a class="sourceLine" id="cb56-2" title="2"><span class="kw">library</span>(e1071)</a>
<a class="sourceLine" id="cb56-3" title="3"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb56-4" title="4"></a>
<a class="sourceLine" id="cb56-5" title="5"><span class="co"># Import data</span></a>
<a class="sourceLine" id="cb56-6" title="6">after_school =<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/zief0002/musings/master/data/after-school.csv&quot;</span>)</a>
<a class="sourceLine" id="cb56-7" title="7"></a>
<a class="sourceLine" id="cb56-8" title="8"><span class="co"># Create plot of KDE</span></a>
<a class="sourceLine" id="cb56-9" title="9"><span class="kw">ggplot</span>(<span class="dt">data =</span> after_school, <span class="kw">aes</span>(<span class="dt">x =</span> delinq)) <span class="op">+</span></a>
<a class="sourceLine" id="cb56-10" title="10"><span class="st">  </span><span class="kw">geom_density</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> treatment), <span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">bw =</span> <span class="dv">3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb56-11" title="11"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb56-12" title="12"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;T-scaled delinquency measure&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb56-13" title="13"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb56-14" title="14"><span class="st">  </span><span class="kw">scale_fill_manual</span>(</a>
<a class="sourceLine" id="cb56-15" title="15">    <span class="dt">name =</span> <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb56-16" title="16">    <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#003366&quot;</span>, <span class="st">&quot;#ffcc00&quot;</span>)</a>
<a class="sourceLine" id="cb56-17" title="17">  )</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-01"></span>
<img src="07-randomization-tests_files/figure-html/fig-01-1.png" alt="Kernel density plots for the distribution of the T-scaled delinquency measure conditioned on treatment group." width="60%" />
<p class="caption">
Figure 1: Kernel density plots for the distribution of the T-scaled delinquency measure conditioned on treatment group.
</p>
</div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1"><span class="co"># Compute summary measures</span></a>
<a class="sourceLine" id="cb57-2" title="2">after_school <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb57-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(treatment) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb57-4" title="4"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb57-5" title="5">    <span class="dt">M =</span> <span class="kw">mean</span>(delinq),</a>
<a class="sourceLine" id="cb57-6" title="6">    <span class="dt">SD =</span> <span class="kw">sd</span>(delinq),</a>
<a class="sourceLine" id="cb57-7" title="7">    <span class="dt">G1 =</span> <span class="kw">skewness</span>(delinq, <span class="dt">type =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb57-8" title="8">    <span class="dt">G2 =</span> <span class="kw">kurtosis</span>(delinq, <span class="dt">type =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb57-9" title="9">    <span class="dt">N =</span> <span class="kw">n</span>()</a>
<a class="sourceLine" id="cb57-10" title="10">  ) </a></code></pre></div>
<pre><code>## # A tibble: 2 x 6
##   treatment     M    SD    G1      G2     N
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;
## 1 Control    54.9 13.7   1.08  0.0973    41
## 2 Treatment  48.0  8.29  3.89 18.2       37</code></pre>
<p>Examination of Figure <a href="KDE.html#fig:fig-01">1</a> reveals similarities in the shape of the distribution for the two groups—both are right skewed. The sample means suggest that the students in the control group had, on average, a level of delinquent behaviors that was 6.9 points higher than the students in the treatment group (<span class="math inline">\(54.9 - 48.0 = 6.9\)</span>). The distribution of scores for students in the treatment group also had less variation, were more skewed, and more peaked (leptokurtic) than the distribution of scores in the control group.</p>
<p>Is this difference within the chance variation one would expect given random assignment? Or is it outside of those expectations? In other words, what is the expected mean difference if there is no effect of after-school programs and different students had been randomly assigned to the treatment and control groups? For this example, there are over <span class="math inline">\(3.93 \times 10^{105}\)</span> permutations of the data! This would take a very long time indeed to list out the possible rearrangements of the data. For this reason, researchers use <em>Monte Carlo simulation</em> to approximate the <em>p</em>-value in situations where the number of permutations is prohibitively time consuming to list.</p>
<p>Monte Carlo simulation is a method that uses a much smaller random sample (say 5000) of all of the random permutations to approximate the reference distribution of the test statistic under inquiry. The approximation is generally very good, and thus, approximate methods are in wide use. At the heart of Monte Carlo methods is the simple idea of selecting a statistical sample to approximate the results rather than to work out the often much more complicated exhaustive solution.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> <span class="citation">Dwass (<a href="#ref-dwass">1957</a>)</span> used the Monte Carlo method to simplify the problem of examining all permutation results and found that it provided a close match to the exact results.</p>
<p><br /><br /></p>
<div id="rerandomization-of-the-data" class="section level3">
<h3>Rerandomization of the Data</h3>
<p>A random permutation of assignment to groups can be carried out using the <code>sample()</code> function. This function samples the values in a vector <em>without replacement</em>. It is comparable to writing each group label on a notecard, shuffling those cards, and then dealing them out into a new order. This results in a random rearrangement of the original groups. The syntax below illustrates how the <code>sample()</code> function can be used to randomly permute the five group labels from the cognitive-behavioral and social problem-solving example given previously.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" title="1"><span class="co"># Create aand view the vector of the original values</span></a>
<a class="sourceLine" id="cb59-2" title="2">original_values =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Treatment&quot;</span>, <span class="st">&quot;Treatment&quot;</span>, <span class="st">&quot;Control&quot;</span>, <span class="st">&quot;Control&quot;</span>)</a>
<a class="sourceLine" id="cb59-3" title="3">original_values</a></code></pre></div>
<pre><code>## [1] &quot;Treatment&quot; &quot;Treatment&quot; &quot;Control&quot;   &quot;Control&quot;</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" title="1"><span class="co"># Randomly permute the group values</span></a>
<a class="sourceLine" id="cb61-2" title="2"><span class="kw">sample</span>(original_values)</a></code></pre></div>
<pre><code>## [1] &quot;Control&quot;   &quot;Treatment&quot; &quot;Treatment&quot; &quot;Control&quot;</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" title="1"><span class="co"># Randomly permute the group values again</span></a>
<a class="sourceLine" id="cb63-2" title="2"><span class="kw">sample</span>(original_values)</a></code></pre></div>
<pre><code>## [1] &quot;Treatment&quot; &quot;Control&quot;   &quot;Treatment&quot; &quot;Control&quot;</code></pre>
<p>Recall the syntax we used earlier to compute the conditional means.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" title="1"><span class="co"># Compute conditional means</span></a>
<a class="sourceLine" id="cb65-2" title="2">after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb65-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(treatment) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb65-4" title="4"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb65-5" title="5">    <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb65-6" title="6">    )</a></code></pre></div>
<pre><code>## # A tibble: 2 x 2
##   treatment     M
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 Control    54.9
## 2 Treatment  48.0</code></pre>
<p>We can also compute the difference in means by piping this output into another <code>summarize()</code> function. Note that the mean values are stored in the column <code>M</code>. We can use the <code>diff()</code> function to find the difference in these values within <code>summarize()</code>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" title="1"><span class="co"># Compute difference between conditional means</span></a>
<a class="sourceLine" id="cb67-2" title="2">after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb67-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(treatment) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb67-4" title="4"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb67-5" title="5">    <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb67-6" title="6">    ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb67-7" title="7"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_diff =</span> <span class="kw">diff</span>(M))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   mean_diff
##       &lt;dbl&gt;
## 1     -6.91</code></pre>
<p>We can use the <code>sample()</code> function to permute the group labels directly in this syntax.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" title="1"><span class="co"># Permute labels and find means</span></a>
<a class="sourceLine" id="cb69-2" title="2">after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb69-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb69-4" title="4"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb69-5" title="5">    <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb69-6" title="6">    )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb69-7" title="7"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean_diff =</span> <span class="kw">diff</span>(M))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   mean_diff
##       &lt;dbl&gt;
## 1     -1.60</code></pre>
<p>This syntax does the following:</p>
<ul>
<li>Permute the group labels of “Treatment” and “Control”</li>
<li>Find the mean for the 41 delinquency scores associated with the permuted “Control” labels and that for the 37 delinquency scores associated with the permuted “Treatment” labels</li>
<li>Compute the difference between these means</li>
</ul>
<p>If we were to re-run the syntax, we obtain a different permutation of the group labels, and hence, a different difference in means.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" title="1"><span class="co"># Permute labels, compute conditional means, and find difference in means</span></a>
<a class="sourceLine" id="cb71-2" title="2">after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb71-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb71-4" title="4"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb71-5" title="5">    <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb71-6" title="6">    )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb71-7" title="7"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb71-8" title="8">    <span class="dt">mean_diff =</span> <span class="kw">diff</span>(M)</a>
<a class="sourceLine" id="cb71-9" title="9">    )</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   mean_diff
##       &lt;dbl&gt;
## 1    -0.250</code></pre>
<p><br /><br /></p>
</div>
<div id="repeating-the-randomization-process" class="section level3">
<h3>Repeating the Randomization Process</h3>
<p>To obtain a Monte Carlo <em>p</em>-value, many random permutations of the data will need to be drawn. For each permutation the mean difference will also need to be computed. Statistical computing tasks often require repeated computations. This can be performed in a number of ways in R. One manner to perform replication is to use a For loop. For loops repeat a set of computations many times. For example consider the following For loop.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</a>
<a class="sourceLine" id="cb73-2" title="2">  </a>
<a class="sourceLine" id="cb73-3" title="3">  <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;In this iteration, i = &quot;</span>, i))</a>
<a class="sourceLine" id="cb73-4" title="4"></a>
<a class="sourceLine" id="cb73-5" title="5">  }</a></code></pre></div>
<pre><code>## [1] &quot;In this iteration, i = 1&quot;
## [1] &quot;In this iteration, i = 2&quot;
## [1] &quot;In this iteration, i = 3&quot;
## [1] &quot;In this iteration, i = 4&quot;
## [1] &quot;In this iteration, i = 5&quot;</code></pre>
<p>For loops are implemented using the <code>for()</code> function. The argument, in this case <code>i = 1:5</code> sets up an object <code>i</code> that takes on the value 1 in the first iteration, 2 in the second iteration, 3 in the third iteration, 4 in the fourth iteration, and 5 in the fifth iteration. The syntax in between the curly braces is being repeated in each iteration. In our example the text “In this iteration i =” is printed follwed by the value for <code>i</code>.</p>
<p>To carry out 10 trials of the randomization test, we can embed the syntax we used to permute the group labels and compute the difference in means within a For loop.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" title="1"><span class="co"># Carry out 10 trials of the randomization</span></a>
<a class="sourceLine" id="cb75-2" title="2"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){</a>
<a class="sourceLine" id="cb75-3" title="3">  </a>
<a class="sourceLine" id="cb75-4" title="4">  <span class="co"># Permute labels, compute conditional means, and find difference in means</span></a>
<a class="sourceLine" id="cb75-5" title="5">  after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb75-6" title="6"><span class="st">    </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb75-7" title="7"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb75-8" title="8">      <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb75-9" title="9">      )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb75-10" title="10"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb75-11" title="11">      <span class="dt">mean_diff =</span> <span class="kw">diff</span>(M)</a>
<a class="sourceLine" id="cb75-12" title="12">      )</a>
<a class="sourceLine" id="cb75-13" title="13"></a>
<a class="sourceLine" id="cb75-14" title="14">  }</a></code></pre></div>
<p>There is one small hitch, which is that we need to include additional syntax that actually will record the difference in means. To do this we will first create an object (prior to running the For loop) that includes 10 blank slots to record the 10 simulated differences in means. Below, weThen, we will assign the output from our computation in the For loop into a particular slot in this empty object. The <code>[i]</code> will put this output into the <em>i</em>th slot of the empty <code>my_sample</code> object. So the output from the first iteration will be placed in the first empty slot of <code>my_sample</code>, etc.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" title="1"><span class="co"># Create object with 10 empty slots</span></a>
<a class="sourceLine" id="cb76-2" title="2">my_sample =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb76-3" title="3"></a>
<a class="sourceLine" id="cb76-4" title="4"></a>
<a class="sourceLine" id="cb76-5" title="5"><span class="co"># Carry out 10 trials of the randomization</span></a>
<a class="sourceLine" id="cb76-6" title="6"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){</a>
<a class="sourceLine" id="cb76-7" title="7">  </a>
<a class="sourceLine" id="cb76-8" title="8">  my_sample[i] =<span class="st"> </span>after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb76-9" title="9"><span class="st">    </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb76-10" title="10"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb76-11" title="11">      <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb76-12" title="12">      )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb76-13" title="13"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb76-14" title="14">      <span class="dt">mean_diff =</span> <span class="kw">diff</span>(M)</a>
<a class="sourceLine" id="cb76-15" title="15">      )</a>
<a class="sourceLine" id="cb76-16" title="16"></a>
<a class="sourceLine" id="cb76-17" title="17">}</a>
<a class="sourceLine" id="cb76-18" title="18"></a>
<a class="sourceLine" id="cb76-19" title="19"><span class="co"># View simulation results</span></a>
<a class="sourceLine" id="cb76-20" title="20">my_sample</a></code></pre></div>
<pre><code>## [[1]]
## [1] 0.06328279
## 
## [[2]]
## [1] 5.061042
## 
## [[3]]
## [1] -4.245485
## 
## [[4]]
## [1] 1.729202
## 
## [[5]]
## [1] 1.389848
## 
## [[6]]
## [1] 0.4026368
## 
## [[7]]
## [1] -2.260778
## 
## [[8]]
## [1] -4.245485
## 
## [[9]]
## [1] -2.584707
## 
## [[10]]
## [1] -0.6051417</code></pre>
<p>The <code>my_sample</code> object is a list (a particular structure in R). To compute on this it is best to coerce this list into a vector by using the <code>unlist()</code> function. We assign this vector a name, and embed it in the <code>data.frame()</code> function. This creates a data frame with a single column that includes the simulated differences in means from the randomization test. We can then use <strong>tidyverse</strong> functionality to create plots or compte numerical summaries of these results.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" title="1"><span class="co"># Create a data frame that includes the results</span></a>
<a class="sourceLine" id="cb78-2" title="2">sim_results =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb78-3" title="3">  <span class="dt">mean_diff =</span> <span class="kw">unlist</span>(my_sample)</a>
<a class="sourceLine" id="cb78-4" title="4">  )</a>
<a class="sourceLine" id="cb78-5" title="5"></a>
<a class="sourceLine" id="cb78-6" title="6"><span class="co"># View data</span></a>
<a class="sourceLine" id="cb78-7" title="7">sim_results</a></code></pre></div>
<pre><code>##      mean_diff
## 1   0.06328279
## 2   5.06104153
## 3  -4.24548451
## 4   1.72920237
## 5   1.38984838
## 6   0.40263678
## 7  -2.26077785
## 8  -4.24548451
## 9  -2.58470666
## 10 -0.60514173</code></pre>
<p>In practice, we carry out many more than 10 trials. Below, we carry out 4,999 trials of the randomization test.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" title="1"><span class="co"># Create object with 4,999 empty slots</span></a>
<a class="sourceLine" id="cb80-2" title="2">my_sample =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="dv">4999</span>)</a>
<a class="sourceLine" id="cb80-3" title="3"></a>
<a class="sourceLine" id="cb80-4" title="4"></a>
<a class="sourceLine" id="cb80-5" title="5"><span class="co"># Set random seed for replication purposes</span></a>
<a class="sourceLine" id="cb80-6" title="6"><span class="kw">set.seed</span>(<span class="dv">1984</span>)</a>
<a class="sourceLine" id="cb80-7" title="7"></a>
<a class="sourceLine" id="cb80-8" title="8"><span class="co"># Carry out 4,999 trials of the randomization</span></a>
<a class="sourceLine" id="cb80-9" title="9"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4999</span>){</a>
<a class="sourceLine" id="cb80-10" title="10">  </a>
<a class="sourceLine" id="cb80-11" title="11">  my_sample[i] =<span class="st"> </span>after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb80-12" title="12"><span class="st">    </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb80-13" title="13"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb80-14" title="14">      <span class="dt">M =</span> <span class="kw">mean</span>(delinq)</a>
<a class="sourceLine" id="cb80-15" title="15">      )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb80-16" title="16"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb80-17" title="17">      <span class="dt">mean_diff =</span> <span class="kw">diff</span>(M)</a>
<a class="sourceLine" id="cb80-18" title="18">      )</a>
<a class="sourceLine" id="cb80-19" title="19"></a>
<a class="sourceLine" id="cb80-20" title="20">}</a>
<a class="sourceLine" id="cb80-21" title="21"></a>
<a class="sourceLine" id="cb80-22" title="22"><span class="co"># Create a data frame that includes the results</span></a>
<a class="sourceLine" id="cb80-23" title="23">sim_results =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb80-24" title="24">  <span class="dt">mean_diff =</span> <span class="kw">unlist</span>(my_sample)</a>
<a class="sourceLine" id="cb80-25" title="25">  )</a>
<a class="sourceLine" id="cb80-26" title="26"></a>
<a class="sourceLine" id="cb80-27" title="27"><span class="co"># View data</span></a>
<a class="sourceLine" id="cb80-28" title="28"><span class="kw">head</span>(sim_results)</a></code></pre></div>
<pre><code>##    mean_diff
## 1  0.4026368
## 2 -0.9342123
## 3 -0.2709295
## 4  1.7343441
## 5  2.7266974
## 6 -3.5873434</code></pre>
<p><br /><br /></p>
</div>
<div id="examining-the-monte-carlo-distribution-and-obtaining-the-p-value" class="section level3">
<h3>Examining the Monte Carlo Distribution and Obtaining the <em>p</em>-Value</h3>
<p>It is always useful to plot the Monte Carlo distribution to assess if the permutations were correctly carried out. The syntax to plot the density of the permuted mean differences is provided below, along with the syntax to summarize this distribution.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1"><span class="co"># Create plot of KDE</span></a>
<a class="sourceLine" id="cb82-2" title="2"><span class="kw">ggplot</span>(<span class="dt">data =</span> sim_results, <span class="kw">aes</span>(<span class="dt">x =</span> mean_diff)) <span class="op">+</span></a>
<a class="sourceLine" id="cb82-3" title="3"><span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb82-4" title="4"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb82-5" title="5"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Difference in means&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb82-6" title="6"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-02"></span>
<img src="07-randomization-tests_files/figure-html/fig-02-1.png" alt="Kernel density plot for the distribution of permuted mean differences. The point represents the observed mean difference of 6.9. The shaded area represents the *p*-value." width="60%" />
<p class="caption">
Figure 3: Kernel density plot for the distribution of permuted mean differences. The point represents the observed mean difference of 6.9. The shaded area represents the <em>p</em>-value.
</p>
</div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" title="1"><span class="co"># Compute summary measures</span></a>
<a class="sourceLine" id="cb83-2" title="2">sim_results <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb83-3" title="3"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb83-4" title="4">    <span class="dt">M =</span> <span class="kw">mean</span>(mean_diff),</a>
<a class="sourceLine" id="cb83-5" title="5">    <span class="dt">SE =</span> <span class="kw">sd</span>(mean_diff),</a>
<a class="sourceLine" id="cb83-6" title="6">    <span class="dt">N =</span> <span class="kw">n</span>()</a>
<a class="sourceLine" id="cb83-7" title="7">  ) </a></code></pre></div>
<pre><code>##            M       SE    N
## 1 0.06004492 2.720253 4999</code></pre>
<p>The plot of the kernel density estimate—shown in Figure <a href="KDE.html#fig:fig-02">3</a> —indicates that the distribution of the 4,999 permuted mean differences is symmetric and unimodal. As seen in Figure <a href="KDE.html#fig:fig-02">3</a>, the mean differences are, on average, close to 0, as this value is at the base of the curve peak. This is confirmed by examining the numerical summaries. Since the delinquency scores were permuted under the assumption of no difference between the two groups, an average mean difference near zero is expected. The standard error of 2.72 is relatively small. This suggests that there is little variation in the mean differences based on only differences in random assignment. Because the standard deviation is quantifying the variation in a statistic (e.g., the mean difference), it is referred to as a <em>standard error</em>.</p>
<p>To obtain the Monte Carlo <em>p</em>-value, recall that the proportion of permuted sample mean differences as extreme or more extreme than the original observed difference of 6.9 needs to be computed. The <code>arrange()</code> function can be used to sort columns in a data frame from the smallest to the largest element. The elements as extreme or more extreme than 6.9 could then be manually counted. This is, of course, quite time consuming—especially when the number of permutations is large.</p>
<p>A better method is to use the <code>tally()</code> function to identify and count these elements. The syntax below shows this method of counting the elements of the <code>mean_diffs</code> column that are greater than or equal to 6.9 and also the number of elements that are less than or equal to <span class="math inline">\(-6.9\)</span>. We can also count both simultaneously by combining both logical statements using the OR operator (<code>|</code>).</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1"><span class="co"># Count number of elements with a value greater than or equal to 6.9</span></a>
<a class="sourceLine" id="cb85-2" title="2">sim_results <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb85-3" title="3"><span class="st">  </span><span class="kw">tally</span>(mean_diff <span class="op">&gt;=</span><span class="st"> </span><span class="fl">6.9</span>)</a></code></pre></div>
<pre><code>##    n
## 1 19</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" title="1"><span class="co"># Count number of elements with a value less than or equal to -6.9</span></a>
<a class="sourceLine" id="cb87-2" title="2">sim_results <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb87-3" title="3"><span class="st">  </span><span class="kw">tally</span>(mean_diff <span class="op">&lt;=</span><span class="st"> </span><span class="fl">-6.9</span>)</a></code></pre></div>
<pre><code>##    n
## 1 27</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="co"># Count both simultaneously</span></a>
<a class="sourceLine" id="cb89-2" title="2">sim_results <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb89-3" title="3"><span class="st">  </span><span class="kw">tally</span>(mean_diff <span class="op">&lt;=</span><span class="st"> </span><span class="fl">-6.9</span> <span class="op">|</span><span class="st"> </span>mean_diff <span class="op">&gt;=</span><span class="st"> </span><span class="fl">6.9</span>)</a></code></pre></div>
<pre><code>##    n
## 1 46</code></pre>
<p>Based on the random permutations of data, 46 of the 4,999 random permutations resulted in a mean difference as extreme or more extreme than the observed mean difference of 6.9. This would seem to suggest that the <em>p</em>-value is</p>
<p><span class="math display">\[
P\biggl(\bigl| \textrm{observed~difference}\bigr| \geq 6.9\biggr)=\frac{46}{4999}=0.009.
\]</span></p>
<p>This proportion is an estimate of the shaded area in Figure <a href="KDE.html#fig:fig-02">3</a>. It turns out that this method of calculating a <em>p</em>-value gives a biased estimate in that it tends to under-estimate the true <em>p</em>-value when using Monte Carlo simulation. This happens because only a random sample of all possible permutations has been taken. <span class="citation">Davison &amp; Hinkley (<a href="#ref-davison">1997</a>)</span> provide a correction to the Monte Carlo <em>p</em>-value as</p>
<p><span class="math display">\[
p=\frac{r+1}{k+1},
\]</span></p>
<p>where <span class="math inline">\(r\)</span> is the number of permutations (replicates) that are as extreme or more extreme than the observed statistic, and <span class="math inline">\(k\)</span> is the number of randomly sampled permutations performed in the Monte Carlo simulation. Using the correction,<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> the Monte Carlo <em>p</em>-value for our example would be</p>
<p><span class="math display">\[
p=\frac{46+1}{4999+1}=0.0094.
\]</span></p>
<p>Based on either the corrected or uncorrected <em>p</em>-value, the null hypothesis is not discarded. That is, the statistical evidence is not particularly strong to rule out no difference between the treatment and control groups. It is important to note that when the number of permutations that are sampled is large, the difference between the value of the corrected and uncorrected <em>p</em>-value is virtually nonexistent, and furthermore, that this correction is not universally carried out <span class="citation">(Boos, <a href="#ref-boos">2003</a>)</span>. Aside from producing a less biased estimate of the true <em>p</em>-value, this correction also avoids the problem of obtaining a <em>p</em>-value of 0 when the observed test statistic is greater than any that were randomly sampled in the simulation, since the minimum possible estimate would be <span class="math inline">\(1/(k+1)\)</span>. Because of this, it is a good habit to always use the adjustment in practice.</p>
<p><br /><br /></p>
</div>
</div>
<div id="validity-of-the-inferences-and-conclusions-drawn-from-a-randomization-test" class="section level2">
<h2>Validity of the Inferences and Conclusions Drawn from a Randomization Test</h2>
<p>The validity of the inferences and conclusions one can draw from any statistical test always need to be evaluated. For example, in order to validly use the <em>p</em>-value obtained from a statistical test as a measure of the strength of evidence against the null hypothesis, there are certain criteria or <em>assumptions</em> that need to be met. These assumptions vary depending on the statistical test that is performed. For the conclusions from the randomization test to be valid, the assumption of <em>exchangeability</em> needs to hold.</p>
<p><br /><br /></p>
<div id="exchangeability" class="section level3">
<h3>Exchangeability</h3>
<p>Exchangeability for the randomization test specifies that, under the null hypothesis, all possible permutations of the data are <em>equally likely</em>. In other words, each permutation of the data is exchangeable with any other permutation of the data. This assumption is required since the computation of the <em>p</em>-value used in the randomization test gives equal weight to each permutation of the data.</p>
<p>In randomized experimental research, whether generalizable or not, the assumption of exchangeability is tenable. The randomization test permutes data in a manner that is consistent with the procedure of random assignment. Thus, the results obtained are valid when random assignment has been initially used in the study. In the after-school example previously considered, exchangeability is assumed to hold, as the participants were randomly assigned to the conditions.</p>
<p><br /><br /></p>
</div>
<div id="nonexperimental-research-permutation-tests" class="section level3">
<h3>Nonexperimental Research: Permutation Tests</h3>
<p>In nonexperimental research studies that do not employ random sampling, the assumption that each permutation of the data is equally likely is a very hard case to make. Some permutations of the data are probably more likely than others. In the absence of random assignment, is it possible to validly draw conclusions from the results of applying this procedure? When the same procedure is applied to data from groups which were randomly sampled rather than randomly assigned, the procedure is called a <em>permutation test</em> rather than a randomization test.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> Oscar Kempthorne, who has written extensively about the theory of randomization, has distinguished between randomization and permutation tests, writing, “the distinction between randomization tests and permutation tests is important. The latter are based on the assumption of random sampling, an assumption that is often patently false or unverifiable, even though necessary to make an attack on the substantive problem being addressed” <span class="citation">(Kempthorne, <a href="#ref-kempthorne">1986</a>, p. 524)</span>.</p>
<p>The use of permutation tests in nonexperimental research is becoming more widespread. When random sampling has been employed, but random assignment to conditions has not been implemented, <span class="citation">Pitman (<a href="#ref-pitman">1937</a>)</span> has provided theoretical justification for the use of permutation methods in producing valid results. An example of random sampling without random assignment is the example comparing the educational achievement between Mexican and non-Mexican immigrants.</p>
<p>The rationale for permutation tests, however, is quite different from that for randomization tests. For nonexperimental research with random sampling, the permutations of data do not provide potential outcomes for the same participants, but rather outcomes for other participants that may have been drawn from hypothetical, infinitely sized, populations. In such cases, although the use of the permutation method is justified, the results and conclusions need to be interpreted with caution. It is no longer the case that the results can be used to claim cause-and-effect arguments.</p>
<p><br /><br /></p>
</div>
<div id="nonexperimental-nongeneralizable-research" class="section level3">
<h3>Nonexperimental, Nongeneralizable Research</h3>
<p>More likely, in education, is the situation in which there is no random sampling and no random assignment. In nonexperimental, nongeneralizable research showing that the assumption of exchangeability has been met is a very difficult exercise. It is not supported via random assignment nor through random sampling. Should one not bother analyzing data in which neither random sampling nor random assignment have taken place? The authors of this monograph take the position that some information is better than no information, even if the information is less precise or need to be qualified in some fashion. <span class="citation">Winch &amp; Campbell (<a href="#ref-winch">1969</a>)</span> argued that although the results from nonexperimental research in which random sampling did not occur cannot be used to identify causes of observed differences, small <em>p</em>-values can help researchers rule out chance differences due to random fluctuations in the data. Such conclusions can be very powerful. They suggest that such conclusions might help a researcher decide whether or not follow-up research might be worth pursuing, or if one should abandon pursuit of the investigation in question. Perhaps the best advice was proffered by Charles Babbage <span class="citation">(in Blackett, <a href="#ref-blackett">1968</a>, p. xiii)</span>, who said, “Errors using inadequate data are much less than those using no data at all.”</p>
<p><br /><br /></p>
</div>
</div>
<div id="generalization-from-the-randomization-results" class="section level2">
<h2>Generalization from the Randomization Results</h2>
<p>In the after-school program example, the data used to examine the effects was obtained from a research design that used random assignment. Based on the design and data collection methods, what are the conclusions that can be drawn and the inferences that can be made regarding after-school programs?</p>
<p>As <span class="citation">Campbell &amp; Stanley (<a href="#ref-campbell">1963</a>, p. 5)</span> point out, typically, educational and behavioral researchers are interested in two types of validity questions, namely (1) whether “the experimental treatments make a difference in this specific experimental instance”; and (2) “to what populations, settings, treatment variables, and measurement variables can this effect be generalized.” These two questions refer to types of validity—<em>internal</em> and <em>external</em>—respectively.</p>
<p>The random assignment of students to conditions allows the researchers to draw conclusions about the effects of the treatment in this particular study—it provides evidence of internal validity. It is unlikely that there are average differences between students who participated in after-school programs and those that did not—there is no effect of after-school program on delinquency—for students who participated in the study.</p>
<p>Do the results from this specific study, no treatment effect of the after-school program, hold for different students in different schools in different communities? The answer to this question is unclear. As <span class="citation">Edgington &amp; Onghena (<a href="#ref-edgington">2007</a>, p. 8)</span> point out, “statistical inferences about populations cannot be made without random samples from those populations…in the absence of random sampling, statistical inferences about treatment effects must be restricted to the participants (or other experimental units) used in an experiment.”</p>
<p>Does this imply that the conclusions regarding the effects of treatment can only be made about the 356 students used in this study? Statistically, the answer to this question is “yes.” However, <em>nonstatistical inferences</em> can be drawn on the basis of logical considerations. Drawing nonstatistical generalizations is a common practice in educational and behavioral research where samples are often not randomly sampled from a particular population <span class="citation">(see @{edgington2; Edgington &amp; Onghena, <a href="#ref-edgington">2007</a>; Shadish et al., <a href="#ref-shadish">2002</a>)</span>. This involves drawing inferences about populations and settings that seem similar to the participants and settings involved in the study conducted, especially in terms of characteristics that appear relevant. For example, a researcher might make nonstatistical inferences about students that have similar educational and socioeconomic backgrounds as those in our study.</p>
<p><br /><br /></p>
</div>
<div id="summarizing-the-findings-2" class="section level2">
<h2>Summarizing the Findings</h2>
<p>When reporting the results from a randomization or permutation test, it is important to report the method used to analyze the data along with the <em>p</em>-value. It is also necessary to report whether the <em>p</em>-value is exact or approximated via Monte Carlo methods. If the latter is the case, the number of data permutations that were carried out should be reported, and whether the correction for the <em>p</em>-value was used. The following is an example that might be used to summarize the results from the after-school program research.</p>
<blockquote>
<p>
Three-hundred fifty-six middle-school students were randomly assigned to either fully participate in an after-school program (<span class="math inline">\(n=169\)</span>), or were given ‘treatment as usual’ (<span class="math inline">\(n=187\)</span>). The treatment as usual students were invited to attend one after-school activity per month. A randomization test was used to determine whether there was a statistically reliable difference in the effect of delinquency between students in these two groups. A Monte Carlo <em>p</em>-value was computed by permuting the data 4,999 times. Using a correction suggested by <span class="citation">Davison &amp; Hinkley (<a href="#ref-davison">1997</a>)</span>, a <em>p</em>-value of <span class="math inline">\(0.009\)</span> was obtained. This is evidence against the null hypothesis of no treatment effect, and may suggest that after-school programs do not contribute to differences in delinquency between students who participate fully and those who do not participate.
</p>
</blockquote>
<p><br /><br /></p>
</div>
<div id="extension-tests-of-the-variance" class="section level2">
<h2>Extension: Tests of the Variance</h2>
<p>In the educational and behavioral sciences, it is often the case that the researcher will test whether the locations of two distributions (i.e., means, medians) are equivalent. Sometimes it can also be more informative to examine whether the variation between two distributions is equivalent. For example, in research related to educational curriculum, comparisons are often made between old and new curricula. It is common for researchers to find no differences in the average achievement between old and new curricula, yet have the achievement scores of the new curricula be smaller than those of the old curricula. This is an example of a treatment effect that manifests itself through changes in variation rather than location. Again, consider the after-school program research. The original null hypothesis was that</p>
<center>
<p>
<span class="math inline">\(H_0:\)</span> The after-school program does not have an effect on delinquency.
</p>
</center>
<p>It was found that in terms of average levels of delinquency, there was some evidence against the null hypothesis. What about in terms of variation? The two samples show differences in variation, with the control group (<span class="math inline">\(s^2=188\)</span>) having more variation in delinquency measures than the treatment group (<span class="math inline">\(s^2=68.7\)</span>). Does the sample difference of 119.3 provide convincing evidence that the after-school program has an effect on the variation of delinquency measures? Specifically, the null hypothesis,</p>
<p><span class="math display">\[
H_0:~\sigma^2_{\mathrm{Control}} = \sigma^2_{\mathrm{Treatment}},
\]</span></p>
<p>is tested. This hypothesis is equivalent to</p>
<p><span class="math display">\[
H_0:~\sigma^2_{\mathrm{Control}} - \sigma^2_{\mathrm{Treatment}} = 0.
\]</span></p>
<p>The same process of simulation introduced earlier in the chapter can be used to obtain a Monte Carlo <em>p</em>-value for this analysis. The only difference is that instead of writing a function to compute the mean difference, the function needs to compute the difference in variances. The syntax below shows how to carry out 4,999 permutations of the data and compute the difference in variances.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1"><span class="co"># Create object with 4999 empty slots</span></a>
<a class="sourceLine" id="cb91-2" title="2">my_sample =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="dv">4999</span>)</a>
<a class="sourceLine" id="cb91-3" title="3"></a>
<a class="sourceLine" id="cb91-4" title="4"></a>
<a class="sourceLine" id="cb91-5" title="5"><span class="co"># Set random seed for replication purposes</span></a>
<a class="sourceLine" id="cb91-6" title="6"><span class="kw">set.seed</span>(<span class="dv">1992</span>)</a>
<a class="sourceLine" id="cb91-7" title="7"></a>
<a class="sourceLine" id="cb91-8" title="8"></a>
<a class="sourceLine" id="cb91-9" title="9"><span class="co"># Carry out 4999 trials of the randomization</span></a>
<a class="sourceLine" id="cb91-10" title="10"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4999</span>){</a>
<a class="sourceLine" id="cb91-11" title="11">  my_sample[i] =<span class="st"> </span>after_school <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb91-12" title="12"><span class="st">    </span><span class="kw">group_by</span>(<span class="kw">sample</span>(treatment)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb91-13" title="13"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb91-14" title="14">      <span class="dt">V =</span> <span class="kw">var</span>(delinq)</a>
<a class="sourceLine" id="cb91-15" title="15">      )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb91-16" title="16"><span class="st">    </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb91-17" title="17">      <span class="dt">var_diff =</span> <span class="kw">diff</span>(V)</a>
<a class="sourceLine" id="cb91-18" title="18">      )</a>
<a class="sourceLine" id="cb91-19" title="19">}</a>
<a class="sourceLine" id="cb91-20" title="20"></a>
<a class="sourceLine" id="cb91-21" title="21"></a>
<a class="sourceLine" id="cb91-22" title="22"><span class="co"># Create a data frame that includes the results</span></a>
<a class="sourceLine" id="cb91-23" title="23">sim_results =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb91-24" title="24">  <span class="dt">var_diff =</span> <span class="kw">unlist</span>(my_sample)</a>
<a class="sourceLine" id="cb91-25" title="25">  )</a>
<a class="sourceLine" id="cb91-26" title="26"></a>
<a class="sourceLine" id="cb91-27" title="27"></a>
<a class="sourceLine" id="cb91-28" title="28"><span class="co"># Count number of permuted differences more extreme than 119.3</span></a>
<a class="sourceLine" id="cb91-29" title="29">sim_results <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb91-30" title="30"><span class="st">  </span><span class="kw">tally</span>(var_diff <span class="op">&lt;=</span><span class="st"> </span><span class="fl">-119.3</span> <span class="op">|</span><span class="st"> </span>var_diff <span class="op">&gt;=</span><span class="st"> </span><span class="fl">119.3</span>)</a></code></pre></div>
<pre><code>##     n
## 1 425</code></pre>
<p>Using the correction for Monte Carlo <em>p</em>-values,</p>
<p><span class="math display">\[
p=\frac{425+1}{4999+1}=0.0852.
\]</span></p>
<p>The probability of obtaining a sample difference in variance as extreme or more extreme than the observed difference of 119.3 is 0.085. This <em>p</em>-value constitutes weak evidence against the null hypothesis and does not portend that there are differences in variation that are attributable to participation in after-school programs.</p>
<p><br /><br /></p>
</div>
<div id="further-reading-3" class="section level2">
<h2>Further Reading</h2>
<p>Major sources on the theory of randomization include <span class="citation">Kempthorne (<a href="#ref-kempthorne2">1955</a>)</span>, <span class="citation">Kempthorne (<a href="#ref-kempthorne3">1977</a>)</span>, and <span class="citation">Fisher (<a href="#ref-fisher2">1935</a>)</span>. <span class="citation">Ernst (<a href="#ref-ernst">2004</a>)</span> provides a readable account of both randomization and permutation methods, including application of such procedures. <span class="citation">Edgington &amp; Onghena (<a href="#ref-edgington">2007</a>)</span> provide an extensive treatise on randomization tests for many situations. An implementation of permutation tests under a unified framework in R is explained in detail in <span class="citation">(<span class="citeproc-not-found" data-reference-id="Hothorn"><strong>???</strong></span>)</span>.</p>
<!-- FIX MATH -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ CommonHTML: { scale: 180 } });
</script>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-blackett">
<p>Blackett, P. M. S. (1968). Address of the president professor P. M. S. Blackett, O.M., C.H., at the anniversary meeting, 30 November 1967. <em>Proceedings of the Royal Society of London. Series B, Biological Sciences</em>, <em>169</em>(1016), v–xviii.</p>
</div>
<div id="ref-boos">
<p>Boos, D. D. (2003). Introduction to the bootstrap world. <em>Statistical Science</em>, <em>18</em>(2), 168–174.</p>
</div>
<div id="ref-campbell">
<p>Campbell, D. T., &amp; Stanley, J. C. (1963). <em>Experimental and quasi-experimental designs for research</em>. Rand-McNally.</p>
</div>
<div id="ref-davison">
<p>Davison, A., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and their application</em>. Cambridge University Press.</p>
</div>
<div id="ref-dwass">
<p>Dwass, M. (1957). Modified randomization tests for nonparametric hypotheses. <em>The Annals of Mathematical Statistics</em>, <em>28</em>, 181–187.</p>
</div>
<div id="ref-edgington">
<p>Edgington, E. S., &amp; Onghena, P. (2007). <em>Randomization tests</em> (4th ed.). Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-ernst">
<p>Ernst, M. (2004). Permutation methods: A basis for exact inference. <em>Statistical Science</em>, <em>19</em>(4), 676–685.</p>
</div>
<div id="ref-fisher4">
<p>Fisher, R. A. (1925). <em>Statistical methods for research workers</em>. Oliver; Boyd.</p>
</div>
<div id="ref-fisher2">
<p>Fisher, R. A. (1935). <em>The design of experiments</em>. Oliver; Boyd.</p>
</div>
<div id="ref-gottfredson">
<p>Gottfredson, D. C., Cross, A., Wilson, D., Rorie, M., &amp; Connell, N. (2010). An experimental evaluation of the All Stars prevention curriculum in a community after school setting. <em>Prevention Science</em>, <em>11</em>(2), 351–379.</p>
</div>
<div id="ref-kempthorne2">
<p>Kempthorne, O. (1955). The randomization theory of experimental inference. <em>Journal of the American Statistical Association</em>, <em>50</em>(271), 946–967.</p>
</div>
<div id="ref-kempthorne3">
<p>Kempthorne, O. (1977). Why randomize? <em>Journal of Statistical Planning and Inference</em>, <em>1</em>(1), 1–25.</p>
</div>
<div id="ref-kempthorne4">
<p>Kempthorne, O. (1979). Sampling inference, experimental inference and observation inference. <em>Sankhya: The Indian Journal of Statistics, Series B</em>, <em>40</em>(3/4), 115–145.</p>
</div>
<div id="ref-kempthorne">
<p>Kempthorne, O. (1986). Randomization—II. In S. Kotz &amp; N. L. Johnson (Eds.), <em>Encyclopedia of statistical sciences</em> (Vol. 7, pp. 519–524). Wiley.</p>
</div>
<div id="ref-light">
<p>Light, R. J., Singer, J. D., &amp; Willett, J. B. (1990). <em>By design: Planning research on higher education</em>. Harvard University Press.</p>
</div>
<div id="ref-pitman">
<p>Pitman, E. J. G. (1937). Significance tests which may be applied to samples from any populations. <em>Supplement to the Journal of the Royal Statistical Society</em>, <em>4</em>(1), 119–130.</p>
</div>
<div id="ref-shadish">
<p>Shadish, W. R., Cook, T. D., &amp; Campbell, D. T. (2002). <em>Experimental and quasi-experimental designs for generalized causal inference</em>. Houghton Mifflin Company.</p>
</div>
<div id="ref-stamps">
<p>Stamps, K., &amp; Bohon, S. A. (2006). Educational attainment in new and established Latino metropolitan destinations. <em>Social Science Quarterly</em>, <em>87</em>(5), 1225–1240.</p>
</div>
<div id="ref-Wasserstein:2019">
<p>Wasserstein, R., &amp; Schirm, A. (2019). <em>Moving to a world beyond <span class="math inline">\(p &lt; .05\)</span></em>. Keynote presentation at the United States Conference on Teaching Statistics.</p>
</div>
<div id="ref-winch">
<p>Winch, R. F., &amp; Campbell, D. T. (1969). Proof? No evidence? Yes. The significance of tests of significance. <em>The American Sociologist</em>, <em>4</em>(May), 140–144.</p>
</div>
<div id="ref-Zieffler:2011">
<p>Zieffler, A. S., Harring, J. R., &amp; Long, J. D. (2011). <em>Comparing groups: Randomization and bootstrap methods using R</em>. Wiley.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>This example was inspired by a similar study carried out by <span class="citation">Lochman et al. (<a href="#ref-lochman">1989</a>)</span>.<a href="randomization-tests.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>Note that the term permutation as used in this context is different from the strictly mathematical definition, which is a reordering of the numbers <span class="math inline">\(1, \ldots, n\)</span> and is computed as <span class="math inline">\(n\)</span>! (read, “<span class="math inline">\(n\)</span> factorial”).<a href="randomization-tests.html#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>In fact, the method was initially proposed by Stanislaw Ulam, in 1946, who wanted to know the probability that a Canfield solitaire laid out with 52 cards would be sucsessful <span class="citation">(Eckhardt, <a href="#ref-eckhardt">1987</a>)</span>. After trying in vain to solve the problem exhaustively through mathematical combinatorics, Ulam laid out several random deals and counted the number of successes.<a href="randomization-tests.html#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>This adjustment is why some researchers prefer to have the sampled number of permutations end in a 9 (e.g., 4,999 or 9,999).<a href="randomization-tests.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>It is important to note that some authors use the two terms synonymously.<a href="randomization-tests.html#fnref19" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="multivariate-exploration.html"><button class="btn btn-default">Previous</button></a>
<a href="final-words.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>

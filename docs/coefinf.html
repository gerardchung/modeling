<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Coefficient-Level Inference | Statistical Modeling and Computation for Educational Scientists" />
<meta property="og:type" content="book" />


<meta property="og:description" content="EPsy 8251 and 8252 Notes" />
<meta name="github-repo" content="zief0002/modeling" />

<meta name="author" content="Andrew Zieffler" />

<meta name="date" content="2020-06-29" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="EPsy 8251 and 8252 Notes">

<title>Coefficient-Level Inference | Statistical Modeling and Computation for Educational Scientists</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="style/style.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/navbar.css" type="text/css" />
<link rel="stylesheet" href="style/notes.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#foreword">Foreword</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="slrd.html#slrd">Simple Linear Regression—Description</a></li>
<li><a href="ordinary-least-squares-ols-estimation.html#ordinary-least-squares-ols-estimation">Ordinary Least Squares (OLS) Estimation</a></li>
<li><a href="cor.html#cor">Correlation and Standardized Regression</a></li>
<li><a href="coefinf.html#coefinf">Coefficient-Level Inference</a></li>
<li><a href="modinf.html#modinf">Model-Level Inference</a></li>
<li><a href="multreg.html#multreg">Introduction to Multiple Regression</a></li>
<li><a href="statcontrol.html#statcontrol">Understanding Statistical Control</a></li>
<li><a href="assumptions.html#assumptions">Distributional Assumptions Underlying the Regression Model</a></li>
<li><a href="dummy.html#dummy">Dummy Coding Categorical Predictors</a></li>
<li><a href="polychotomous.html#polychotomous">Polychotomous Categorical Predictors</a></li>
<li><a href="interaction-01.html#interaction-01">Introduction to Interaction Effects</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="coefinf" class="section level1">
<h1>Coefficient-Level Inference</h1>
<p>In this chapter, you will learn about statistical inference at the coefficient-level for regression models. To do so, we will use the <a href="https://raw.githubusercontent.com/zief0002/modeling/master/data/keith-gpa.csv">keith-gpa.csv</a> data to examine whether time spent on homework is related to GPA. The data contain three attributes collected from a random sample of <span class="math inline">\(n=100\)</span> 8th-grade students (see the <a href="http://zief0002.github.io/epsy-8251/codebooks/keith-gpa.html">data codebook</a>). To begin, we will load several libraries and import the data into an object called <code>keith</code>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="coefinf.html#cb54-1"></a><span class="co"># Load libraries</span></span>
<span id="cb54-2"><a href="coefinf.html#cb54-2"></a><span class="kw">library</span>(broom)</span>
<span id="cb54-3"><a href="coefinf.html#cb54-3"></a><span class="kw">library</span>(corrr)</span>
<span id="cb54-4"><a href="coefinf.html#cb54-4"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb54-5"><a href="coefinf.html#cb54-5"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb54-6"><a href="coefinf.html#cb54-6"></a><span class="kw">library</span>(readr)</span>
<span id="cb54-7"><a href="coefinf.html#cb54-7"></a></span>
<span id="cb54-8"><a href="coefinf.html#cb54-8"></a><span class="co"># Read in data</span></span>
<span id="cb54-9"><a href="coefinf.html#cb54-9"></a>keith =<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&quot;https://raw.githubusercontent.com/zief0002/modeling/master/data/keith-gpa.csv&quot;</span>)</span>
<span id="cb54-10"><a href="coefinf.html#cb54-10"></a><span class="kw">head</span>(keith)</span></code></pre></div>
<pre><code># A tibble: 6 x 3
    gpa homework parent_ed
  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    78        2        13
2    79        6        14
3    79        1        13
4    89        5        13
5    82        3        16
6    77        4        13</code></pre>
<p>In the <a href="slrd.html#slrd">previous chapters</a>, we examined the marginal distributions of the outcome (<code>gpa</code>) and predictor (<code>homework</code>), as well as statistics summarizing the mean and variation in these distributions. We also examine a scatterplot of the outcome versus each of the predictors and the bivariate correlations (<a href="cor.html#cor">see here</a>). Here we use the <code>ggMarginal()</code> function from the <a href="https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html">ggExtra</a> package to include the marginal density plots directly on the scatterplot.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="coefinf.html#cb56-1"></a><span class="co"># Scatterplot</span></span>
<span id="cb56-2"><a href="coefinf.html#cb56-2"></a>p1 =<span class="st"> </span><span class="kw">ggplot</span>( <span class="dt">data =</span> keith, <span class="kw">aes</span>(<span class="dt">x =</span> homework, <span class="dt">y =</span> gpa) ) <span class="op">+</span></span>
<span id="cb56-3"><a href="coefinf.html#cb56-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb56-4"><a href="coefinf.html#cb56-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb56-5"><a href="coefinf.html#cb56-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Time spent on homework per week (in hours)&quot;</span>) <span class="op">+</span></span>
<span id="cb56-6"><a href="coefinf.html#cb56-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;GPA (on a 100-pt. scale)&quot;</span>)</span>
<span id="cb56-7"><a href="coefinf.html#cb56-7"></a></span>
<span id="cb56-8"><a href="coefinf.html#cb56-8"></a><span class="co"># Load ggExtra package</span></span>
<span id="cb56-9"><a href="coefinf.html#cb56-9"></a><span class="kw">library</span>(ggExtra)</span>
<span id="cb56-10"><a href="coefinf.html#cb56-10"></a></span>
<span id="cb56-11"><a href="coefinf.html#cb56-11"></a><span class="co"># Plot scatterplot and density plots on single graph</span></span>
<span id="cb56-12"><a href="coefinf.html#cb56-12"></a><span class="kw">ggMarginal</span>(p1, <span class="dt">margins =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-3-1.png" alt="Density plots of the marginal distributions of GPA and time spent on homework. The scatterplot showing the relationship between GPA and time spent on homework is also shown." width="50%" />
<p class="caption">
Figure 1: Density plots of the marginal distributions of GPA and time spent on homework. The scatterplot showing the relationship between GPA and time spent on homework is also shown.
</p>
</div>
<p>We also create a table to present the correlation coefficients. This table could be extended to include additional variables (which we will do in later chapters). To save creating an additional table, we also include the mean and standard deviations in the same table by placing them on the main diagonal.</p>
<table style="width:60%; margin-left: auto; margin-right: auto;" class="table">
<caption>
<span id="tab:unnamed-chunk-4">Table 7: </span>Correlations between 8th-Grade students’ GPA and weekly time spent on homework. Means and standard deviations (in parentheses) are displayed on the main diagonal.
</caption>
<thead>
<tr>
<th style="text-align:left;text-align: center;">
Measure
</th>
<th style="text-align:center;text-align: center;">
1
</th>
<th style="text-align:center;text-align: center;">
2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<ol style="list-style-type: decimal">
<li>GPA
</td>
<td style="text-align:center;">
80.47 (7.62)
</td>
<td style="text-align:center;">
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="2" style="list-style-type: decimal">
<li>Time spent on homework
</td>
<td style="text-align:center;">
.33
</td>
<td style="text-align:center;">
5.09 (2.06)
</td>
</tr>
</tbody>
</table></li>
</ol></li>
</ol>
<p>We will also fitted a model by regressing GPA on time spent on homework and storing those results in an object called <code>lm.1</code>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="coefinf.html#cb57-1"></a><span class="co"># Fit regression model</span></span>
<span id="cb57-2"><a href="coefinf.html#cb57-2"></a>lm<span class="fl">.1</span> =<span class="st"> </span><span class="kw">lm</span>(gpa <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>homework, <span class="dt">data =</span> keith)</span>
<span id="cb57-3"><a href="coefinf.html#cb57-3"></a>lm<span class="fl">.1</span></span></code></pre></div>
<pre><code>
Call:
lm(formula = gpa ~ 1 + homework, data = keith)

Coefficients:
(Intercept)     homework  
     74.290        1.214  </code></pre>
<p>The fitted equation is:</p>
<p><span class="math display">\[
\hat{\mathrm{GPA}_i} = 74.29 + 1.21(\mathrm{Time~spent~on~homework}_i),
\]</span></p>
<p>Summarizing this,</p>
<blockquote>
<p>This model estimated mean GPA for all 8th-grade students who spend 0 hours a week on homework is 74.29. Each additional hour 8th-grade students spend per week on homework is associated with a difference in GPA of 1.21, on average. Differences in time spent on homework explains 10.7% of the variation in students’ GPAs. All this suggests that time spent on homework is related to GPA for the <span class="math inline">\(n=100\)</span> 8th-graders in the sample.</p>
</blockquote>
<p><br /></p>
<div id="statistical-inference" class="section level2">
<h2>Statistical Inference</h2>
<p>What if we want to understand the relationship between time spent on homework and GPA for a larger population of 8th-grade students, say all of them in the district? The problem is that if we had drawn a different sample of <span class="math inline">\(n=100\)</span> 8th-grade students, all the regression estimates (<span class="math inline">\(\hat\beta_0, \hat\beta_1,\)</span> and <span class="math inline">\(R^2\)</span>) would be different than the ones we obtained from our sample. This makes it difficult to say, for example, how does the conditional mean GPA differs for students with differing amounts of time spent on homework. In our observed sample, <span class="math inline">\(\hat\beta_1\)</span> was 1.21. But, had we sampled different students, we might have found that <span class="math inline">\(\hat\beta_1\)</span> was 2.03. And a different random sample of employees we might have produced a <span class="math inline">\(\hat\beta_1\)</span> of 0.96.</p>
<p>This variation in the estimates arises because of the random nature of the sampling. One of the key findings in statistical theory is that the amount of variation in estimates under random sampling is completely predictable (this variation is called <a href="https://en.wikipedia.org/wiki/Sampling_error">sampling error</a>). Being able to quantify the sampling error allows us to provide a more informative answer to the research question. For example, it turns out that based on the quantification of sampling error in our example, we believe that the actual <span class="math inline">\(\beta_1\)</span> is between 0.51 and 1.92.</p>
<p>Statistical inference allows us to learn from incomplete or imperfect data <span class="citation">Gelman &amp; Hill (<a href="#ref-Gelman:2007" role="doc-biblioref">2007</a>)</span>. In many studies, the primary interest is to learn about one or more characteristics about a population. These characteristics must be estimated from sample data. This is the situation in our example, where we have only a sample of 8th-grade students and we want to understand the relationship between time spent on homework and GPA for ALL 8th-grade students in the district.</p>
<p>In the example, the variation in estimates arises because of sampling variation. It is also possible to have variation because of imperfect measurement. This is called <a href="https://en.wikipedia.org/wiki/Observational_error">measurement error</a>. Despite these being very different sources of variation, in practice they are often combined (e.g., we measure imperfectly and we want to make generalizations).</p>
<p>Regardless of the sources of variation, the goals in most regression analyses are two-fold:</p>
<ol style="list-style-type: decimal">
<li>Estimate the parameters from the observed data; and</li>
<li>Summarize the amount of uncertainty (e.g., quantify the sampling error) in those estimates.</li>
</ol>
<p>The first goal we addressed in the <a href="slrd.html#slrd">Simple Linear Regression—Description</a> chapter. It is the second goal that we will explore in this chapter.</p>
<p><br /></p>
</div>
<div id="quantification-of-uncertainty" class="section level2">
<h2>Quantification of Uncertainty</h2>
<p>Before we talk about estimating uncertainty in regression, let me bring you back in time to your <em>Stat I</em> course. In that course, you probably spent a lot of time talking about sampling variation for the mean. The idea went something like this: Imagine you have a population that is infinitely large. The observations in this population follow some probability distribution. (This distribution is typically unknown in practice, but for now, let’s pretend we know what that distribution is.) For our purposes, let’s assume the population is normally distributed with a mean of <span class="math inline">\(\mu\)</span> and a standard deviation of <span class="math inline">\(\sigma\)</span>.</p>
<p>Sample <span class="math inline">\(n\)</span> observations from that population. Based on the <span class="math inline">\(n\)</span> sampled observations, find the mean. We will call this <span class="math inline">\(\hat\mu_1\)</span> since it is an estimate for the population mean (the subscript just says it is the first sample). In all likelihood, <span class="math inline">\(\hat\mu_1\)</span> is not the exact same value as <span class="math inline">\(\mu\)</span>. It varies from the population mean because of sampling error.</p>
<p>Now, sample another <span class="math inline">\(n\)</span> observations from the population. Again, find the mean. We will call this estimate <span class="math inline">\(\hat\mu_2\)</span>. Again, it probably varies from <span class="math inline">\(\mu\)</span>, and may be different than <span class="math inline">\(\hat\mu_1\)</span> as well. Continue to repeat this process: randomly sample <span class="math inline">\(n\)</span> observations from the population; and find the mean.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="figs/notes-06-thought-experiment-means.png" alt="Thought experiment for sampling samples of size n from the population to obtain the sampling distribution of the mean." width="80%" />
<p class="caption">
Figure 13: Thought experiment for sampling samples of size n from the population to obtain the sampling distribution of the mean.
</p>
</div>
<p>The distribution of the sample means, it turns out, is quite predictable using statistical theory. Theory predicts that the distribution of the sample means will be normally distributed. It also predicts that the mean, or <a href="https://en.wikipedia.org/wiki/Expected_value">expected value</a>, of all the sample means will be equal to the population mean, <span class="math inline">\(\mu\)</span>. Finally, theory predicts that the standard deviation of this distribution, called the <a href="https://en.wikipedia.org/wiki/Standard_error">standard error</a>, will be equal to the population standard deviation divided by the square root of the sample size. Mathematically, we would write all this as,</p>
<p><span class="math display">\[
\hat\mu_n\sim\mathcal{N}\left(\mu, \dfrac{\sigma}{\sqrt{n}}\right).
\]</span></p>
<p>The important thing is not that you memorize this result, but that you understand that <strong>the process of randomly sampling from a known population can lead to predictable results in the distribution of statistical summaries</strong> (e.g., the distribution of sample means). The other crucial thing is that there the sampling variation can be quantified. The standard error is the quantification of that sampling error. In this case, it gives a numerical answer to the question of how variable the sample mean will be because of random sampling.</p>
<p><br /></p>
<div id="quantification-of-uncertainty-in-regression" class="section level3">
<h3>Quantification of Uncertainty in Regression</h3>
<p>We can extend these ideas to regression. Now the thought experiment goes something like this: Imagine you have a population that is infinitely large. The observations in this population have two attributes, call them <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The relationship between these two attributes can be expressed via a regression equation as: <span class="math inline">\(\hat{Y}=\beta_0 + \beta_1(X)\)</span>. Randomly sample <span class="math inline">\(n\)</span> observations from the population. This time, rather than computing a mean, regress the sample <span class="math inline">\(Y\)</span> values on the sample <span class="math inline">\(X\)</span> values. Since the sample regression coefficients are estimates of the population parameters, we will write this as: <span class="math inline">\(\hat{Y}=\hat{\beta}_{0,1} + \hat{\beta}_{1,1}(X)\)</span>. Repeat the process. This time the regression equation is: <span class="math inline">\(\hat{Y}=\hat{\beta}_{0,2} + \hat{\beta}_{1,2}(X)\)</span>. Continue this process an infinite number of times.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="figs/notes-06-thought-experiment-coefficients.png" alt="Thought experiment for sampling samples of size n from the population to obtain the sampling distribution of the regression coefficients." width="80%" />
<p class="caption">
Figure 3: Thought experiment for sampling samples of size n from the population to obtain the sampling distribution of the regression coefficients.
</p>
</div>
<p>Statistical theory again predicts the characteristics of the two distributions, that of <span class="math inline">\(\hat{\beta}_0\)</span> and that of <span class="math inline">\(\hat{\beta}_1\)</span>. The distribution of <span class="math inline">\(\hat{\beta}_0\)</span> can be expressed as,</p>
<p><span class="math display">\[
\hat\beta_0\sim\mathcal{N}\left(\beta_0,~ \sigma_\epsilon\sqrt{\dfrac{1}{n} + \dfrac{\mu_X^2}{\sum(X_i-\mu_X)^2}}\right).
\]</span></p>
<p>Similarly, the distribution of <span class="math inline">\(\hat{\beta}_1\)</span> can be expressed as,</p>
<p><span class="math display">\[
\hat\beta_1\sim\mathcal{N}\left(\beta_1,~ \dfrac{\sigma_\epsilon}{\sigma_x\sqrt{n-1}}\right).
\]</span></p>
<p>Again, don’t panic over the formulae. What is important is that theory allows us to quantify the variation in both <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that is due to sampling error. In practice, our statistical software will give us the numerical estimates of the two standard errors.</p>
<p><br /></p>
</div>
<div id="obtaining-ses-for-the-regression-coefficients" class="section level3">
<h3>Obtaining SEs for the Regression Coefficients</h3>
<p>To obtain the standard errors for the regression coefficients, we will use the <code>tidy()</code> function from the <strong>broom</strong> package to display the fitted regression output. We provide the fitted regression object as the input to this function.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="coefinf.html#cb59-1"></a><span class="co"># Display the coefficient-level output</span></span>
<span id="cb59-2"><a href="coefinf.html#cb59-2"></a><span class="kw">tidy</span>(lm<span class="fl">.1</span>)</span></code></pre></div>
<pre><code># A tibble: 2 x 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)    74.3      1.94      38.3  1.01e-60
2 homework        1.21     0.354      3.43 8.85e- 4</code></pre>
<p>In the displayed output, we now obtain the estimates for the standard errors in addition to the coefficient estimates. We can use these values to quantify the amount of uncertainty due to sampling error. For example, the estimate for the slope, 1.21, has a standard error of 0.35. One way to envision this is as a distribution. Our best guess (mean) for the slope parameter is 1.21. The standard deviation of this distribution is 0.35, which indicates the precision (uncertainty) of our estimate.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-9-1.png" alt="Sampling distribution of the slope coefficient. The distribution is approximately normal with a mean of 1.21 and a standard error of 0.35." width="50%" />
<p class="caption">
Figure 5: Sampling distribution of the slope coefficient. The distribution is approximately normal with a mean of 1.21 and a standard error of 0.35.
</p>
</div>
<p>In the social sciences, it is typical to express uncertainty as <span class="math inline">\(\pm2(SE)\)</span>. Here we would say that becuase of sampling variation, the slope is likely between 0.51 and 1.91. Interpreting this, we might say,</p>
<blockquote>
<p>For all 8th-graders in the district, each one-hour difference in time spent on homework per week is associated with a difference in overall GPA between 0.51 and 1.91, on average.</p>
</blockquote>
<p>Similarly, we could express the uncertainty in the intercept as,</p>
<p><span class="math display">\[
74.29 \pm 2(1.94) = \left[70.41,~78.17\right]
\]</span></p>
<p>Interpreting this, we might say,</p>
<blockquote>
<p>The average GPA for all 8th-grade students in the district who spend zero hopurs per week on homework is between 70.41 and 78.17.</p>
</blockquote>
<p>We can use the <code>conf.int=TRUE</code> argument in the <code>tidy()</code> function to obtain these limits directly. By default this will compute a 95% CI. This can be changed using the <code>conf.level=</code> argument.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="coefinf.html#cb61-1"></a><span class="kw">tidy</span>(lm<span class="fl">.1</span>, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code># A tibble: 2 x 7
  term        estimate std.error statistic  p.value conf.low conf.high
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)    74.3      1.94      38.3  1.01e-60   70.4       78.1 
2 homework        1.21     0.354      3.43 8.85e- 4    0.512      1.92</code></pre>
<p><br /></p>
</div>
</div>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis Testing</h2>
<p>Some research questions point to examining whether the value of some regression parameter differs from a specific value. For example, it may be of interest whether a particular population model (e.g., one where <span class="math inline">\(\beta_1=0\)</span>) could produce the sample result of a particular <span class="math inline">\(\hat\beta_1\)</span>. To test something like this, we state the value we want to test in a statement called a [null hypothesis](<a href="https://en.wikipedia.org/wiki/Null_hypothesis" class="uri">https://en.wikipedia.org/wiki/Null_hypothesis</a>. For example,</p>
<p><span class="math display">\[
H_0: \beta_1 = 0
\]</span></p>
<p>The hypothesis is a statement about the population. Here we hypothesize <span class="math inline">\(\beta_1=0\)</span>. It would seem logical that one could just examine the estimate of the parameter from the observed sample to answer this question, but we also have to account for sampling uncertainty. The key is to quantify the sampling variation, and then see if the sample result is unlikely given the stated hypothesis.</p>
<p>One question of interest may be: Is there evidence that the average GPA differs for different amounts of time spent on homework? In our example, we have a <span class="math inline">\(\hat\beta_1=1.21\)</span>. This is sample evidence, but does 1.21 differ from 0 more than we would expect because of random sampling? If it doesn’t, we cannot really say that the average GPA differs for different amounts of time spent on homework. To test this, we make an assumption that there is no relationship between time spent on homework and GPA, in other words, the slope of the line under this assumption would be 0. Before we talk about how to test this, we need to introduce one wrinkle into the procedure.</p>
<p><br /></p>
</div>
<div id="estimating-variation-from-sample-data-no-longer-normal" class="section level2">
<h2>Estimating Variation from Sample Data: No Longer Normal</h2>
<p>In theory, the sampling distribution for two regression coefficients were both normally distributed. This is the case when we know the variation parameters in the population. For example, for the sampling distribution of the slope to be normally distributed, we would need to know <span class="math inline">\(\sigma_\epsilon\)</span> and <span class="math inline">\(\sigma_x\)</span>.</p>
<p>In practice these values are typically unknown and are estimated from the sample data. Anytime we are estimating things we introduce additional uncertainty. In this case, the uncertainty affects the shape of the sampling distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-11-1.png" alt="Comparison of two distributions. The normal distribution (solid, blue) and one with additional uncertainty (dashed, orange)." width="50%" />
<p class="caption">
Figure 6: Comparison of two distributions. The normal distribution (solid, blue) and one with additional uncertainty (dashed, orange).
</p>
</div>
<p>Compare the normal distribution (solid, blue) to the distribution with additional uncertainty (dashed, orange). From the figure you can see that the additional uncertainty slightly changed the shape of the distribution from normal.</p>
<ul>
<li>It is still symmetric and unimodal (like the normal distribution).</li>
<li>The additional uncertainty makes more extreme values more likely than they are in the normal distribution.</li>
<li>The additional uncertainty makes values in the middle less likely than they are in the normal distribution.</li>
</ul>
<p>It is important to note that the amount of uncertainty affects how closely the shape of the distribution matches the normal distribution. And, that the sample size directly affects the amount of uncertainty we have. All things being equal, we have less uncertainty when we have larger samples. The following figure illustrates this idea.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-12-1.png" alt="The normal distribution (solid, blue) and two with additional uncertainty; one based on n=10 (dashed, orange), and the other based on n=4 (dotted, green)." width="50%" />
<p class="caption">
Figure 7: The normal distribution (solid, blue) and two with additional uncertainty; one based on n=10 (dashed, orange), and the other based on n=4 (dotted, green).
</p>
</div>
<p><br /></p>
<div id="the-t-distribution" class="section level3">
<h3>The t-Distribution</h3>
<p>As pointed out, the distributions with uncertainty introduced from using a sample of data are not normally distributed. Thus, it doesn’t make sense to use a normal distribution as a model for describing the sampling variation. Instead, we will a <em>t</em>-distribution; a family of distributions that have several advantageous properties:</p>
<ul>
<li>The are unimodal and symmetric.</li>
<li>The have more variation (uncertainty) than the normal distribution resulting in a distribution that has thicker tails and is shorter in the middle than a normal distribution.</li>
<li>How thick the tails are and how short the middle of the distribution is, is related to the sample size.</li>
</ul>
<p>Specifically, the <em>t</em>-distribution is unimodal and symmetric with a mean of 0. The variance of the distribution (which also specifies the exact shape), is</p>
<p><span class="math display">\[
\mathrm{Var} = \frac{\textit{df}}{\textit{df} - 2}
\]</span></p>
<p>for <span class="math inline">\(\textit{df}&gt;2\)</span> where <span class="math inline">\(\textit{df}\)</span> is referred to as the <a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">degrees of freedom</a>.</p>
<p><br /></p>
</div>
<div id="back-to-the-hypothesis-test" class="section level3">
<h3>Back to the Hypothesis Test</h3>
<p>Recall that we are interested in testing the following hypotheis,</p>
<p><span class="math display">\[
H_0: \beta_1 = 0
\]</span></p>
<p>To test this we compute the number of standard errors that our observed slope (<span class="math inline">\(\hat\beta_1=1.21\)</span>) is from the hypothesized value of zero (stated in the null hypothesis). Since we already obtained the standard error for the slope (<span class="math inline">\(SE=0.354\)</span>), we just use some straight-forward algebra to compute this:</p>
<p><span class="math display">\[
\frac{1.21 - 0}{0.354} = 3.42
\]</span></p>
<p>Interpreting this, we can say,</p>
<blockquote>
<p>The observed slope of 1.21 is 3.42 standard errors from the expected value of 0.</p>
</blockquote>
<p>This value is referred to as the observed <em>t</em>-value. (It is similar to a <em>z</em>-value in the way it is computed; it is standardizing the distance from the observed slope to the hypothesized value of zero. But, since we had to estimate the SE using the data, we introduced additional uncertainty; hence a <em>t</em>-value.)</p>
<p>We can evaluate this <em>t</em>-value within the appropriate <em>t</em>-distribution. For regression coefficients, the <em>t</em>-distribution we will use for evaluation has degrees of freedom that are a function of the sample size and the number of coefficients being estimated in the regression model, namely,</p>
<p><span class="math display">\[
\textit{df} = n - (\textrm{number of coefficients}).
\]</span></p>
<p>In our example the sample size (<span class="math inline">\(n\)</span>) is 100, and the number of coefficients being estimated in the regression model is two (<span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>). Thus,</p>
<p><span class="math display">\[
\textit{df} = 100 - 2 = 98
\]</span></p>
<p>Based on this, we will evaluate our observed <em>t</em>-value of 3.42 using a <em>t</em>-distribution with 98 degrees of freedom. Using this distribution, we can compute the probability of obtaining a <em>t</em>-value (under random sampling) at least as extreme as the one in the data under the assumed model. This is equivalent to finding the area under the probability curve for the <em>t</em>-distribution that is greater than or equal to 3.42.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> This is called the <em>p</em>-value.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-13-1.png" alt="Plot of the probability curve for the t(98) distribution. The shaded area under the curve represents the p-value for a two-tailed test evaluating whether the population slope is zero using an observed t-value of 73.42." width="50%" />
<p class="caption">
Figure 14: Plot of the probability curve for the t(98) distribution. The shaded area under the curve represents the p-value for a two-tailed test evaluating whether the population slope is zero using an observed t-value of 73.42.
</p>
</div>
<p>The <em>p</em>-value is computed for us and displayed in the <code>tidy()</code> output, along with the <em>t</em>-value (provided in the <code>statistic</code> column). In our example, <span class="math inline">\(p=0.000885\)</span>. (Note that the <em>p</em>-value might be printed in scientific notation. For example, it may be printed as <code>8.85e-04</code>, which is equivalent to <span class="math inline">\(8.85 \times 10^{-4}\)</span>.) To interpret this we would say,</p>
<blockquote>
<p>The probability of observing a <em>t</em>-value of 3.42, or a <em>t</em>-value that is more extreme, under the assumption that <span class="math inline">\(\beta_1=0\)</span> is 0.000885.</p>
</blockquote>
<p>This is equivalent to saying:</p>
<blockquote>
<p>The probability of observing a sample slope of 1.21, or a slope that is more extreme, under the assumption that <span class="math inline">\(\beta_1=0\)</span> is 0.000885.</p>
</blockquote>
<p>This is quite unlikely, and indicates that the empirical data are inconsistent with the hypothesis that <span class="math inline">\(\beta_1=0\)</span>. As such, it serves as <strong>evidence against the hypothesized model</strong>. In other words, it is likely that <span class="math inline">\(\beta_1\neq0\)</span>.</p>
<p><br /></p>
</div>
<div id="testing-the-intercept" class="section level3">
<h3>Testing the Intercept</h3>
<p>The hypothesis being tested for the intercept is <span class="math inline">\(H_0:\beta_0=0\)</span>. The <code>tidy()</code> output also provides information about this test:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="coefinf.html#cb63-1"></a><span class="kw">tidy</span>(lm<span class="fl">.1</span>)</span></code></pre></div>
<pre><code># A tibble: 2 x 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)    74.3      1.94      38.3  1.01e-60
2 homework        1.21     0.354      3.43 8.85e- 4</code></pre>
<p>The results indicate that the observed intercept of 74.28 is 38.26 standard errors from the hypothesized value of 0;</p>
<p><span class="math display">\[
t = \frac{74.28 - 0}{1.94} = 38.26
\]</span></p>
<p>Assuming the null hypothesis that <span class="math inline">\(\beta_0=0\)</span> is true, the probability of observing a sample intercept of 74.28 or one that more extreme, is <span class="math inline">\(1.01 \times 10^{-60}\)</span>. (Any <em>p</em>-value less than .001 is typically reported as <span class="math inline">\(p&lt;.001\)</span>.) This is evidence against the hypothesized model. Because of this, we would say the empirical data are inconsistent with the hypothesis that <span class="math inline">\(\beta_0=0\)</span>; it is unlikely that the intercept in the population is zero.</p>
<p><br /></p>
</div>
</div>
<div id="statistical-significance-an-outdated-idea-for-research" class="section level2">
<h2>‘Statistical Significance’: An Outdated Idea for Research</h2>
<p>You may have read papers or taken statistics courses that emphasized the language “statistically significant”. This adjective was typically used when the empirical evidence was inconsistent with a hypothesized model, and the researcher subsequently “rejected the null hypothesis”. In the social sciences this occurred when the <span class="math inline">\(p\)</span>-value was less than or equal to 0.05.</p>
<p>In 2019, the American Statistical Association put out a special issue in one of their premier journals, stating,</p>
<div class="actualquote">
<p>…it is time to stop using the term ‘statistically significant’ entirely. Nor should variants such as ‘significantly different,’ ‘p &lt; 0.05,’ and ‘nonsignificant’ survive, whether expressed in words, by asterisks in a table, or in some other way. Regardless of whether it was ever useful, a declaration of ‘statistical significance’ has today become meaningless. <span class="citation">(Wasserstein &amp; Schirm, <a href="#ref-Wasserstein:2019" role="doc-biblioref">2019</a>, p. 2)</span></p>
</div>
<p>They went on to say,</p>
<div class="actualquote">
<p>…no p-value can reveal the plausibility, presence, truth, or importance of an association or effect. Therefore, a label of statistical significance does not mean or imply that an association or effect is highly probable, real, true, or important. Nor does a label of statistical nonsignificance lead to the association or effect being improbable, absent, false, or unimportant. <span class="citation">(Wasserstein &amp; Schirm, <a href="#ref-Wasserstein:2019" role="doc-biblioref">2019</a>, p. 2)</span></p>
</div>
<p>This is not to say that <em>p</em>-values should not be reported; they should. But rather that we should not arbitrarily dichotomize a continuous measure into two categories whose labels are at best meaningless and at worst misleading. The goal of scientific inference (which is much broader than statistical inference for a single study) is replicability and empirically generalizable results and findings. And, as <span class="citation">Hubbard et al. (<a href="#ref-Hubbard:2019" role="doc-biblioref">2019</a>)</span> point out, declaring findings as ‘significant’ or ‘not significant’ works in direct opposition to the broader culmination of knowledge and evidence in a field.</p>
<p>Instead, we want to begin to see the <em>p</em>-value as a measure of incompatibility between the empirical data and a very specific model, one in which a certain set of assumptions are true. Both the empirical data (which are unique to the specific study) and the model’s set of assumptions often make the <em>p</em>-value unique to the specific study carried out and less useful in the broader goal of scientific inference. As such we need to come to view the <em>p</em>-value for what it is, one measure of evidence, for one very particular model, in one very localized study. As Ron Wasserstein reminds us,</p>
<div class="actualquote">
<p>Small p-values are like a right-swipe in Tinder. It means you have an interest. It doesn’t mean you’re ready to book the wedding venue.</p>
</div>
<p><img src="figs/pvalue-and-tinder.jpg" width="60%" style="display: block; margin: auto;" /></p>
<p><br /></p>
</div>
<div id="confidence-intervals-as-compatibility-intervals" class="section level2">
<h2>Confidence Intervals as Compatibility Intervals</h2>
<p>To build on this, let’s return to the reporting and interpretation of confidence intervals. In our example, the 95% CI was:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="coefinf.html#cb65-1"></a><span class="kw">tidy</span>(lm<span class="fl">.1</span>, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code># A tibble: 2 x 7
  term        estimate std.error statistic  p.value conf.low conf.high
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)    74.3      1.94      38.3  1.01e-60   70.4       78.1 
2 homework        1.21     0.354      3.43 8.85e- 4    0.512      1.92</code></pre>
<p>One way of interpreting this interval is that every value in the interval is a parameter value that is reasonably compatible with the empirical data. For example, in considering the CI for the slope parameter, population slope <span class="math inline">\((\beta_1)\)</span> values between 0.51 and 1.92 are all reasonably compatible with the empirical data (with the caveat that, again, all the assumptions used to create the interval are satisfied). As applied researchers, we should describe the practical implications of all values inside the interval, especially the observed effect (or point estimate) and the limits.</p>
<p>For us this means describing the practical implications of the true slope being 1.21, as low as 0.51, and as high as 1.92. Are these meaningful differences in GPA (measure on a 100-pt. scale)? Given that the SD for GPA was 7.62, a one-hour difference in time spent on homework is associated with at most a 0.25 SD difference in GPA or as little as a 0.07 SD difference in GPA. This is not a large difference, however whether it is meaningful depends on previous research about GPA.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
<p>Confidence intervals help us keep an open-mind about uncertainty, after all they suggest several values that are compatible with the empirical data. However, they can also be mislading. <span class="citation">Amrhein et al. (<a href="#ref-Amrhein:2019" role="doc-biblioref">2019</a>)</span> point out four key points fo us to remember as we use CIs:</p>
<ul>
<li>Just because the interval gives the values most compatible with the data, given the assumptions, it does not mean values outside it are incompatible; they are just less compatible.</li>
<li>Not all values inside are equally compatible with the data, given the assumptions. The point estimate is the most compatible, and values near it are more compatible than those near the limits.</li>
<li>Like the 0.05 threshold from which it came, the default 95% used to compute intervals is itself an arbitrary convention.</li>
<li>Last, and most important of all, be humble: compatibility assessments hinge on the correctness of the statistical assumptions used to compute the interval. In practice, these assumptions are at best subject to considerable uncertainty.</li>
</ul>
<p><br /></p>
<div id="coefficient-plot" class="section level3">
<h3>Coefficient Plot</h3>
<p>One plot that helps visualize the estimates of the regression coefficients and the associated uncertainty is a <em>coefficient plot</em>. This plot, recommended by <span class="citation">Gelman &amp; Hill (<a href="#ref-Gelman:2007" role="doc-biblioref">2007</a>)</span>, is a graphical representation of the information provided in the <code>tidy()</code> output.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-17-1.png" alt="Coefficient plot for the model regressing GPA on time spent on homework. Uncertainty based on the 95\% confidence intervals are displayed." width="70%" />
<p class="caption">
Figure 15: Coefficient plot for the model regressing GPA on time spent on homework. Uncertainty based on the 95% confidence intervals are displayed.
</p>
</div>
<p>The coefficient plot shows the estimates of the regression coefficients (dots) and the uncertainty in those estimates via the confidence intervals (blue shading). Notice that darker shading is associated with parameter values that are more probable given the empirical data; the sample estimates are the most likely values for the regression parameters.</p>
<p>To create a coefficient plot, we will use the <code>dwplot()</code> function from the <a href="https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html">dotwhisker</a> package. This function takes the ouput of <code>tidy()</code> as input. Since the function will identify the model, we also mutate a column called <code>model</code> into the tidy output giving the name of the model. Since <code>dwplot()</code> is based on <code>ggplot()</code> syntax, we can add layers to customize the plot in the same manner as if we were building a ggplot. The syntax I used to create the coefficient plot above is:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="coefinf.html#cb67-1"></a><span class="co"># Load library</span></span>
<span id="cb67-2"><a href="coefinf.html#cb67-2"></a><span class="kw">library</span>(dotwhisker)</span>
<span id="cb67-3"><a href="coefinf.html#cb67-3"></a></span>
<span id="cb67-4"><a href="coefinf.html#cb67-4"></a><span class="co"># Store output from tidy</span></span>
<span id="cb67-5"><a href="coefinf.html#cb67-5"></a>mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">tidy</span>(lm<span class="fl">.1</span>) <span class="op">%&gt;%</span></span>
<span id="cb67-6"><a href="coefinf.html#cb67-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&quot;Model A&quot;</span>)</span>
<span id="cb67-7"><a href="coefinf.html#cb67-7"></a></span>
<span id="cb67-8"><a href="coefinf.html#cb67-8"></a><span class="co"># Create plot</span></span>
<span id="cb67-9"><a href="coefinf.html#cb67-9"></a><span class="kw">dwplot</span>(mod_<span class="dv">1</span>, <span class="dt">show_intercept =</span> <span class="ot">TRUE</span>) <span class="op">+</span></span>
<span id="cb67-10"><a href="coefinf.html#cb67-10"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb67-11"><a href="coefinf.html#cb67-11"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name =</span> <span class="st">&quot;Model&quot;</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#c62f4b&quot;</span>)) <span class="op">+</span></span>
<span id="cb67-12"><a href="coefinf.html#cb67-12"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Estimate&quot;</span>) <span class="op">+</span></span>
<span id="cb67-13"><a href="coefinf.html#cb67-13"></a><span class="st">  </span><span class="kw">scale_y_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;Coefficients&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Time spent</span><span class="ch">\n</span><span class="st">on homework&quot;</span>, <span class="st">&quot;Intercept&quot;</span>))</span></code></pre></div>
<p>By default, the plot will display 95% CI. To display a different level of confidence, specify <code>conf.level=</code> argument in <code>tidy()</code>. There are several variations of this plot. For example, below I omit the intercept from this plot. To do this, I use the <code>filter()</code> function to omit the row in the <code>tidy()</code> output that includes the intercept.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> I also change the limits on the <em>x</em>-axis to better fit the homework interval.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="coefinf.html#cb68-1"></a><span class="co"># Omit intercept</span></span>
<span id="cb68-2"><a href="coefinf.html#cb68-2"></a>mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">tidy</span>(lm<span class="fl">.1</span>) <span class="op">%&gt;%</span></span>
<span id="cb68-3"><a href="coefinf.html#cb68-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&quot;Model A&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb68-4"><a href="coefinf.html#cb68-4"></a><span class="st">  </span><span class="kw">filter</span>(term <span class="op">==</span><span class="st"> &quot;homework&quot;</span>)</span>
<span id="cb68-5"><a href="coefinf.html#cb68-5"></a></span>
<span id="cb68-6"><a href="coefinf.html#cb68-6"></a><span class="co"># Create plot</span></span>
<span id="cb68-7"><a href="coefinf.html#cb68-7"></a><span class="kw">dwplot</span>(mod_<span class="dv">1</span>, <span class="dt">show_intercept =</span> <span class="ot">TRUE</span>) <span class="op">+</span></span>
<span id="cb68-8"><a href="coefinf.html#cb68-8"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb68-9"><a href="coefinf.html#cb68-9"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">name =</span> <span class="st">&quot;Model&quot;</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#c62f4b&quot;</span>)) <span class="op">+</span></span>
<span id="cb68-10"><a href="coefinf.html#cb68-10"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;Estimate&quot;</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb68-11"><a href="coefinf.html#cb68-11"></a><span class="st">  </span><span class="kw">scale_y_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;Coefficients&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Time spent</span><span class="ch">\n</span><span class="st">on homework&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="07-coefficient-level-inference_files/figure-html/unnamed-chunk-19-1.png" alt="Coefficient plot for the model regressing GPA on time spent on homework. Uncertainty based on the 95\% confidence interval is displayed. Note that the intercept has been omitted." width="80%" />
<p class="caption">
Figure 16: Coefficient plot for the model regressing GPA on time spent on homework. Uncertainty based on the 95% confidence interval is displayed. Note that the intercept has been omitted.
</p>
</div>
<p><br /></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Amrhein:2019">
<p>Amrhein, V., Greenland, S., &amp; McShane, B. (2019). Retire statistical significance. <em>Nature</em>, <em>567</em>, 305–307.</p>
</div>
<div id="ref-Gelman:2007">
<p>Gelman, A., &amp; Hill, J. (2007). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge University Press.</p>
</div>
<div id="ref-Hubbard:2019">
<p>Hubbard, R., Haig, B. D., &amp; Parsa, R. A. (2019). The limited role of formal statistical inference in scientific inference. <em>The American Statistician</em>, <em>73</em>, 91–98. <a href="https://doi.org/10.1080/00031305.2018.1464947">https://doi.org/10.1080/00031305.2018.1464947</a></p>
</div>
<div id="ref-Wasserstein:2019">
<p>Wasserstein, R., &amp; Schirm, A. (2019). <em>Moving to a world beyond <span class="math inline">\(p &lt; .05\)</span></em>. Keynote presentation at the United States Conference on Teaching Statistics.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>The actual limits from the 95% CI are computed using a multiplier that is slightly different than two; thus the discrepancy between our off-the-cuff computation earlier and the result from <code>tidy()</code>. Using a multiplier of two is often close enough for practical purposes, especially when the sample size is large.<a href="coefinf.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>We actually compute the area under the probability curve that is greater than or equal to 3.42 AND that is less than or equal to <span class="math inline">\(-3.42\)</span>.<a href="coefinf.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>It turns out this is quite a complicated question and the effects of homework depend on a variety of student factors, including age, culture, household income, etc. Many studies have also found a non-linear effect of homework, indicating there may be an optimum amount for some groups of students.<a href="coefinf.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>The intercept could also be dropped using <code>show.intercept=FALSE</code> which is the default for <code>dwplot()</code>. The <code>filter()</code> method, however, allows you to drop or select different predictors for display as well as the intercept.<a href="coefinf.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="cor.html"><button class="btn btn-default">Previous</button></a>
<a href="modinf.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>

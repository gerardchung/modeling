<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Distributional Assumptions Underlying the Regression Model | Statistical Modeling and Computation for Educational Scientists" />
<meta property="og:type" content="book" />


<meta property="og:description" content="EPsy 8251 and 8252 Notes" />
<meta name="github-repo" content="zief0002/modeling" />

<meta name="author" content="Andrew Zieffler" />

<meta name="date" content="2020-06-10" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="EPsy 8251 and 8252 Notes">

<title>Distributional Assumptions Underlying the Regression Model | Statistical Modeling and Computation for Educational Scientists</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="style/style.css" type="text/css" />
<link rel="stylesheet" href="style/table-styles.css" type="text/css" />
<link rel="stylesheet" href="style/syntax.css" type="text/css" />
<link rel="stylesheet" href="style/navbar.css" type="text/css" />
<link rel="stylesheet" href="style/notes.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#foreword">Foreword</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="slrd.html#slrd">Simple Linear Regression—Description</a></li>
<li><a href="ordinary-least-squares-ols-estimation.html#ordinary-least-squares-ols-estimation">Ordinary Least Squares (OLS) Estimation</a></li>
<li><a href="cor.html#cor">Correlation and Standardized Regression</a></li>
<li><a href="coefinf.html#coefinf">Coefficient-Level Inference</a></li>
<li><a href="modinf.html#modinf">Model-Level Inference</a></li>
<li><a href="multreg.html#multreg">Introduction to Multiple Regression</a></li>
<li><a href="statcontrol.html#statcontrol">Understanding Statistical Control</a></li>
<li><a href="assumptions.html#assumptions">Distributional Assumptions Underlying the Regression Model</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="assumptions" class="section level1">
<h1>Distributional Assumptions Underlying the Regression Model</h1>
<p>In this chapter, you will learn about the various distributional assumptions underlying the regression model. To do so, we will return to the <a href="https://raw.githubusercontent.com/zief0002/modeling/master/data/keith.csv">keith-gpa.csv</a> data to examine whether time spent on homework is related to GPA (see the <a href="http://zief0002.github.io/epsy-8251/codebooks/keith-gpa.html">data codebook</a>). To begin, we will load several libraries and import the data into an object called <code>keith</code>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="assumptions.html#cb125-1"></a><span class="co"># Load libraries</span></span>
<span id="cb125-2"><a href="assumptions.html#cb125-2"></a><span class="kw">library</span>(corrr)</span>
<span id="cb125-3"><a href="assumptions.html#cb125-3"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb125-4"><a href="assumptions.html#cb125-4"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb125-5"><a href="assumptions.html#cb125-5"></a><span class="kw">library</span>(patchwork)</span>
<span id="cb125-6"><a href="assumptions.html#cb125-6"></a><span class="kw">library</span>(readr)</span>
<span id="cb125-7"><a href="assumptions.html#cb125-7"></a></span>
<span id="cb125-8"><a href="assumptions.html#cb125-8"></a><span class="co"># Read in data</span></span>
<span id="cb125-9"><a href="assumptions.html#cb125-9"></a>keith =<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&quot;https://raw.githubusercontent.com/zief0002/modeling/master/data/keith-gpa.csv&quot;</span>)</span>
<span id="cb125-10"><a href="assumptions.html#cb125-10"></a><span class="kw">head</span>(keith)</span></code></pre></div>
<pre><code># A tibble: 6 x 3
    gpa homework parent_ed
  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1    78        2        13
2    79        6        14
3    79        1        13
4    89        5        13
5    82        3        16
6    77        4        13</code></pre>
<p>We will also fit a simple regression model that uses time spent on homework to predict variation in GPAs.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="assumptions.html#cb127-1"></a><span class="co"># Fit simple regression model</span></span>
<span id="cb127-2"><a href="assumptions.html#cb127-2"></a>lm<span class="fl">.1</span> =<span class="st"> </span><span class="kw">lm</span>(gpa <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>homework, <span class="dt">data =</span> keith)</span></code></pre></div>
<p><br /></p>
<div id="four-distributional-assumptions-needed-for-validity-of-regression-results" class="section level2">
<h2>Four Distributional Assumptions Needed for Validity of Regression Results</h2>
<p>Recall that the simple regression model is expressed as:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1(X_i) + \epsilon_i
\]</span></p>
<p>There are several distributional assumptions we make about the errors (<span class="math inline">\(\epsilon_i\)</span> values) in the regression model in order for the results we obtain from fitting this model (e.g., coefficient estimates, <span class="math inline">\(p\)</span>-values, CIs) to be valid.</p>
<ul>
<li>[L]inearity</li>
<li>[I]ndependence</li>
<li>[N]ormality</li>
<li>[E]qual variances (Homoskedasticity)</li>
</ul>
<p>You can remember these assumptions using the mnemonic LINE. To better understand these assumptions, imagine that we had the population of <em>X</em>- and <em>Y</em>-values in which all the distributional assumptions were valid. Now imagine we plotted the ordered pairs, <span class="math inline">\((x_i,y_i)\)</span>, and we also regressed the <em>Y</em>-values on the <em>X</em>-values and plotted this regression line as well. A visual depiction of this is shown in Figure 1.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="figs/regression-assumptions.png" alt="A visual depiction of *X*- and *Y*-values and regression line from a population in which the distributional assumptions are valid." width="50%" />
<p class="caption">
Figure 9: A visual depiction of <em>X</em>- and <em>Y</em>-values and regression line from a population in which the distributional assumptions are valid.
</p>
</div>
<p>In Figure 1, the normal distributions depicted are the distribution of <em>Y</em>-values at each value of <em>X</em>, or what we refer to as the conditional distributions of <em>Y</em>. Although only three conditional distributions are shown in Figure 1, there is a conditional distribution for EVERY value of <em>X</em>.</p>
<p>Although the distributional assumptions are about the model’s errors, we can also apply the assumptions to the conditional <span class="math inline">\(Y\)</span>-values since they are <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformations</a> of the error terms. This allows us to use Figure 1 to expand upon each of the distributional assumptions listed earlier.</p>
<ul>
<li><strong>Linearity:</strong> The linearity assumption implies that the MEAN values of <span class="math inline">\(Y\)</span> from all the conditional distributions all fall on the same line. If this is the case, we would say that the conditional mean <span class="math inline">\(Y\)</span>-values are linear.</li>
<li><strong>Independence:</strong> This is not shown in the figure. The assumption is that each <span class="math inline">\(Y\)</span>-value in a particular conditional distribution is independent from every other <span class="math inline">\(Y\)</span>-value in that same distribution.</li>
<li><strong>Normality:</strong> This assumption indicates that every one of the conditional distributions of <span class="math inline">\(Y\)</span>-values is normally distributed.</li>
<li><strong>Equal variances:</strong> This assumption says that the variance (or standard deviation) of all of the conditional distributions is exactly the same.</li>
</ul>
<p><br /></p>
<div id="distributional-assumptions-are-really-about-the-residuals" class="section level3">
<h3>Distributional Assumptions are Really About the Residuals</h3>
<p>Stating these distributional assumptions in terms of the the conditional distributions of <span class="math inline">\(Y\)</span> was useful in helping us visualize them within a typical representation of the regression model through the relationship between <span class="math inline">\(X\)</span>- and <span class="math inline">\(Y\)</span>-values. Technically, however, all the distributional assumptions are about the conditional distributions of the residuals.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Think about how we compute the residuals:</p>
<p><span class="math display">\[
\epsilon_i = Y_i - \hat{Y}_i
\]</span>
In Figure 1, the <span class="math inline">\(\hat{Y}_i\)</span> value is the <span class="math inline">\(Y\)</span>-value that corresponds to the point on the line. Within each conditional distribution of <em>Y</em>, the <span class="math inline">\(\hat{Y}_i\)</span> is constant; in other words all of the observations with the same <em>X</em>-value will have the same <span class="math inline">\(\hat{Y}_i\)</span>-value. That means within a conditional distribution, to compute the residual values, we are subtracting a constant:</p>
<p><span class="math display">\[
\epsilon_i = Y_i - C
\]</span></p>
<p>Remember from the <a href="cor.html#cor">chapter on standardized regression</a>, subtracting a constant from each value in a distribution shifts the center of the distribution. Pick any conditional distribution from Figure 1, which is a normal distribution centered at the <span class="math inline">\(\hat{Y}_i\)</span> value. Now subtract the <span class="math inline">\(\hat{Y}_i\)</span>-value from each <em>Y</em>-value. This will re-center the normal distribution at 0. Thus, the conditional distribution of residuals is normally distributed, has a mean of 0, and has the same variance (or standard deviation) as the conditional distribution of <em>Y</em>-values. If we transform every <em>Y</em>-value in the population, from Figure 1, to a residual value, and re-plot them, the visual depiction now looks like this.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="figs/regression-assumptions-residuals.png" alt="A visual depiction of the simple regression model's assumptions about the residuals." width="50%" />
<p class="caption">
Figure 2: A visual depiction of the simple regression model’s assumptions about the residuals.
</p>
</div>
<p>So if we restate the assumptions in terms of the residuals and the conditional distributions of the residuals,</p>
<ul>
<li><strong>Linearity:</strong> The MEAN value of each of the conditional distributions of the residuals is 0.</li>
<li><strong>Independence:</strong> Again, this is not shown in the figure. The assumption is that each residual value in a particular conditional distribution is independent from every other residual value in that same distribution.</li>
<li><strong>Normality:</strong> This assumption indicates that each of the conditional distributions of residuals is normally distributed.</li>
<li><strong>Equal variance:</strong> The variance (or standard deviation) of all of the conditional distributions of residuals is exactly the same.</li>
</ul>
<p>These assumptions can also be expressed mathematically as,</p>
<p><span class="math display">\[
\epsilon_{i|X} \overset{\mathrm{i.i.d~}}{\sim} \mathcal{N}\left(0, \sigma^2\right)
\]</span></p>
<p>The “i.i.d” stands for <em>independent and identically distributed</em>. The mathematical expression says the residuals conditioned on <em>X</em> (having the same <em>X</em>-value) are independent and identically normally distributed with a mean of 0 and some variance (<span class="math inline">\(\sigma^2\)</span>).</p>
<p><br /></p>
</div>
</div>
<div id="evaluating-the-distributional-assumptions" class="section level2">
<h2>Evaluating the Distributional Assumptions</h2>
<p>Before beginning to evaluate the distributional assumptions using our empirical data, it is important to point out that the assumptions are about the residuals <em>in the population</em>. Because in most analyses, we only have a sample of data, we can never really evaluate whether these assumptions are true. We can only offer a guess as to whether they are tenable given the data we see. The strongest arguments for justification for meeting any of the distibutional assumptions is a theoretical argument based on existing literature in the discipline.</p>
<p><br /></p>
<div id="linearity" class="section level3">
<h3>Linearity</h3>
<p>The linearity assumption is critical in specifying the structural part of the model. Fitting a linear model when the TRUE relationship between <em>X</em> and <em>Y</em> is non-linear may be quite problematic. Coefficients may be wrong. Predictions may also be wrong, especially at the extreme values for <em>X</em>. More importantly, mis-specified models lead to misinformed understandings of the world.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-6-1.png" alt="The left-hand plot shows observations simulated from a nonlinear model. The right-hand plot shows the same data and the results of fitting a linear model to non-linear data. Using the linear fitted model to make predictions would be quite misleading, especially at extreme values of *X*." width="90%" />
<p class="caption">
Figure 13: The left-hand plot shows observations simulated from a nonlinear model. The right-hand plot shows the same data and the results of fitting a linear model to non-linear data. Using the linear fitted model to make predictions would be quite misleading, especially at extreme values of <em>X</em>.
</p>
</div>
<p>In the left-hand plot of Figure 3, when the correct nonlinear model is fitted to the data, the conditional <em>Y</em>-values are scattered above and below the line at each <em>X</em>-value. In the right-hand plot of Figure 3, when a linear model was fitted to data generated from a non-linear function, the data tend to either be consistently above, or below the line, depending on the <em>X</em>-value. This type of systematic deviation would be evidence that the linearity assumption is not tenable. When evaluating this assumption, we want to see data in the scatterplot that is “equally” above and below the fitted regression line at each value of <em>X</em>.</p>
<p>Since the linearity assumption also means that the average residual is 0, if we are evaluating this assumption by looking at a plot of the residuals, we would want to see residuals above (positive) and below (negative) 0. Figure 4 shows scatterplots of the residuals versus the <em>X</em>-values from the two fitted lines. In the left-hand plot, when the residuals are based on the true model, we see that the residuals are scattered above and below 0 at each value of <em>X</em>. In the right-hand plot, in which the residuals were computed based on a mis-specified linear model, we again see that the residuals are clustered above, or below 0, depending on the value of <em>X</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-7-1.png" alt="The left-hand plot shows the residuals from the true nonlinear model versus the *X*-values. The right-hand plot shows the residuals from the mis-specified linear model versus the *X*-values. A reference line at $Y=0$ has also been added to the plot to aid interpretation." width="90%" />
<p class="caption">
Figure 3: The left-hand plot shows the residuals from the true nonlinear model versus the <em>X</em>-values. The right-hand plot shows the residuals from the mis-specified linear model versus the <em>X</em>-values. A reference line at <span class="math inline">\(Y=0\)</span> has also been added to the plot to aid interpretation.
</p>
</div>
<p><br /></p>
</div>
<div id="independence" class="section level3">
<h3>Independence</h3>
<p>The definition of independence relies on formal mathematics. Loosely speaking a set of observations is independent if knowing that one observation is above (or below) the mean value in a conditional distribution conveys no information about whether any other observation in the same conditional distribution is above (or below) its mean value. If observations are not independent, we say they are <em>dependent</em> or <em>correlated</em>.</p>
<p>Independence is not an assumption we can check graphically. To evaluate this assumption we need to know something about the how the data were collected or assigned to values of <em>X</em>. Using random chance in the design of the study, to either select observations (random sampling) or assign them to levels of the predictor (random assignment) will guarantee independence of the observations. Outside of this, independence is often difficult to guarantee, and often has to be a logical argument.</p>
<p>There are a few times that we can ascertain that the independence assumption would be violated. These instances often result from aspects of the data collection process. One such instance common to social science research is when the observations (i.e., cases, subjects) are collected within a physical or spatial proximity to one another. For example, this is typically the case when a researcher gathers a convenience sample based on location, such as sampling students from the same school. Another violation of independence occurs when observations are collected over time (longitudinally), especially when the observations are repeated measures from the same subjects.</p>
<p>One last violation of independence occurs when the observation level used to assign cases to the different predictor values (e.g., treatment or control) does not correspond to the observation level used in the analysis. For example, in educational studies whole classrooms are often assigned to treatment or control. That means that the cases used in the analysis, in order to satisfy the independence assumption, would need to be at the classroom level (e.g., the cases would need to be classroom means), not individual students. This can be deleterious for sample size.</p>
<p>If the independence assumption is violated, almost every value you get in the <code>tidy()</code> and <code>glance()</code> output—the standard errors, <em>t</em>-values, <em>F</em>-values, <em>p</em>-values, and residual standard errors (RMSE)—are wrong. If you suspect that the independence assumption is violated, then you will need to use a method (not OLS regression) that accommodates non-independence.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p><br /></p>
</div>
<div id="normality-and-equal-variances" class="section level3">
<h3>Normality and Equal Variances</h3>
<p>The assumption about normality is about the conditional distribution of errors at each value of <em>X</em>. This assumption is less critical than the assumptions of linearity and independence. It is only problematic for the OLS regression results if there are egregious violations of normality. In general, if the violations of these assumptions are only minor, the results of the OLS regression are still valid; we would say the results from an OLS regression are <em>robust</em> to violations of normality. Even if the violations are bad, there are many transformations of the data that can alleviate this problem.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<p>Normal distributions are symmetric with the density of observations close to the mean. This means that in the scatterplot of the residuals versus the <em>X</em>-values, we want to see symmetry around 0 at each <em>X</em>-value and that most of the residuals are “close” to 0 (“Close” is hard to define as it depends on the standard deviation, remember that 68% of the residuals should be within one standard deviation, 95% within two standard deviations, etc.)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-8-1.png" alt="The left-hand plot shows conditional distributions of normally distributed residuals. The right-hand plot shows conditional distributions that are not normally distributed. The line $Y=0$ has also been included to aid interpretation." width="90%" />
<p class="caption">
Figure 4: The left-hand plot shows conditional distributions of normally distributed residuals. The right-hand plot shows conditional distributions that are not normally distributed. The line <span class="math inline">\(Y=0\)</span> has also been included to aid interpretation.
</p>
</div>
<p>The residuals in the left-hand plot of Figure 4 are symmetric around 0 for each <em>X</em>-value. The bulk of the residuals at each <em>X</em>-value is near 0 and they become less dense the further from 0 they are, for both the positive and negative residual values. This is the pattern we want to see in empirical residuals. The residuals in the right-hand plot of Figure 4, are not symmetric around 0. They are also more dense in the negative values and then become less dense for higher positive values. This is evidence that the normality assumption is violated.</p>
<p>In the examples given in Figure 4, the number of observations and the shape of the conditional distributions make deviations from normality easier to spot. Evaluating the normality assumption in empirical data, which are often composed of fewer observations, is much more of a challenge. Moreover, evaluating the shape of distributions in a scatterplot is not an easy task.</p>
<p>Researchers often evaluate the normality assumption by examining the shape of the marginal distribution of the residuals. Figure 5 shows density plots of the marginal distributions for the same two sets of residuals plotted in Figure 4.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-9-1.png" alt="The left-hand plot shows conditional distributions of normally distributed residuals. The right-hand plot shows conditional distributions that are not normally distributed. The line $Y=0$ has also been included to aid interpretation." width="90%" />
<p class="caption">
Figure 5: The left-hand plot shows conditional distributions of normally distributed residuals. The right-hand plot shows conditional distributions that are not normally distributed. The line <span class="math inline">\(Y=0\)</span> has also been included to aid interpretation.
</p>
</div>
<p>The marginal distribution in the right-hand plot clearly shows deviation from normality and we could safely say that the normality assumption is violated. We may be tempted to say that the marginal distribution in the left-hand plot also violates the assumption of normality as the density plot does not look normal. This would be a mistake. Remember that the assumptions are about the residuals in the population; the sample residuals may deviate from normality simply because of sampling error. Moreover, this looks like a minor violation of the normality assumption and is probably not an issue for the regression results.</p>
</div>
<div id="equal-variances" class="section level3">
<h3>Equal Variances</h3>
<p>Similar to the assumption about normality, the assumption of equal variances (homoskedasticity) is is less critical than the assumptions of linearity and independence, and only egregious violations of the assumption is problematic for the validity of the regression results.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-10-1.png" alt="The left-hand plot shows conditional distributions of normally distributed residuals with equal variances. The center plot shows conditional distributions that are not normally distributed but still have equal variances. The right-hand plot shows conditional distributions that are normally distributed but have unequal variances. The line $Y=0$ has also been included in all plots to aid interpretation." width="99%" />
<p class="caption">
Figure 10: The left-hand plot shows conditional distributions of normally distributed residuals with equal variances. The center plot shows conditional distributions that are not normally distributed but still have equal variances. The right-hand plot shows conditional distributions that are normally distributed but have unequal variances. The line <span class="math inline">\(Y=0\)</span> has also been included in all plots to aid interpretation.
</p>
</div>
<p>When evaluating the assumption of equal variances, we want to see that the range of residual values at each <em>X</em>-value is roughly the same. For the left-hand and center plots in Figure 6, the range of residual values is roughly the same at each <em>X</em>-value, so we can conclude that the equal variances assumption is tenable. In the right-hand plot, the residuals show a pattern of variances that seems to be increasing for larger <em>X</em>-values. That is, the range of the residual values at smaller <em>X</em>-values is smaller than the range of the residual values at larger <em>X</em>-values. This is a violation of the equal variances assumption.</p>
<p><br /></p>
</div>
</div>
<div id="empirically-evaluating-the-distributional-assumptions" class="section level2">
<h2>Empirically Evaluating the Distributional Assumptions</h2>
<p>We can use the residuals computed from the empirical data to evaluate the distributional assumptions of linearity, normality, and equal variances. (The assumption of independence is difficult to evaluate using the data, and is better left to a logical argument that refers to the study design.) Recall that the assumptions are about the residuals. To compute the residuals, we will use the <code>augment()</code> function from the <strong>broom</strong> package. We will also write those results into an object, <code>out_1</code>, so we can compute on it later.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="assumptions.html#cb128-1"></a><span class="co"># Load library</span></span>
<span id="cb128-2"><a href="assumptions.html#cb128-2"></a><span class="kw">library</span>(broom)</span>
<span id="cb128-3"><a href="assumptions.html#cb128-3"></a></span>
<span id="cb128-4"><a href="assumptions.html#cb128-4"></a><span class="co"># Augment the model to get residuals</span></span>
<span id="cb128-5"><a href="assumptions.html#cb128-5"></a>out_<span class="dv">1</span> =<span class="st"> </span><span class="kw">augment</span>(lm<span class="fl">.1</span>)</span>
<span id="cb128-6"><a href="assumptions.html#cb128-6"></a></span>
<span id="cb128-7"><a href="assumptions.html#cb128-7"></a><span class="co"># View augmented data</span></span>
<span id="cb128-8"><a href="assumptions.html#cb128-8"></a><span class="kw">head</span>(out_<span class="dv">1</span>)</span></code></pre></div>
<pre><code># A tibble: 6 x 8
    gpa homework .fitted .resid .std.resid   .hat .sigma  .cooksd
  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
1    78        2    76.7  -1.28      0.180 0.0328   7.28 0.000550
2    79        6    81.6   2.57     -0.358 0.0120   7.27 0.000776
3    79        1    75.5  -3.50      0.495 0.0500   7.27 0.00646 
4    89        5    80.4  -8.64      1.20  0.0100   7.22 0.00728 
5    82        3    77.9  -4.07      0.568 0.0204   7.26 0.00336 
6    77        4    79.1   2.15     -0.298 0.0128   7.27 0.000579</code></pre>
<p>The cases in the augmented data frame are in the same order as the cases in the <code>keith</code> data frame. For example, the first case had a GPA of 78 and spent 2 hours a week on homework. The augmented data also includes several other useful measures for evaluating regression models. For now, we will focus on the <code>.fitted</code> column and the <code>.resid</code> column. Those columns contain the fitted values (<span class="math inline">\(\hat{Y_i}\)</span>) and the residuals for each case based on the model fitted in <code>lm.1</code>.</p>
<p>We will examine two residual plots to help us evaluate the tenability of the assumptions: (1) a density plot of the marginal distribution of residuals, and (2) a scatterplot of the residuals versus the <em>X</em>-values. The density plot will allow us to eval;uate the normality assumption, and the scatterplot will allow us to evaluate the linearity and equal variances assumption. As we make these evaluations, remember that we do not have the entire population of residuals (we obtained our residuals by fitting a regression to a sample of data), so we do not expect that our residuals will actually meet the assumptions perfectly (remember, sampling error). Examining the sample residuals, is however, a reasonable way to evaluate the tenability of assumptions in practice. We just have to keep in mind that the sample residuals may deviate a bit from these assumptions.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="assumptions.html#cb130-1"></a><span class="co"># Density plot of the residuals</span></span>
<span id="cb130-2"><a href="assumptions.html#cb130-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">1</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .resid)) <span class="op">+</span></span>
<span id="cb130-3"><a href="assumptions.html#cb130-3"></a><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>) <span class="op">+</span></span>
<span id="cb130-4"><a href="assumptions.html#cb130-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb130-5"><a href="assumptions.html#cb130-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Residual&quot;</span>) <span class="op">+</span></span>
<span id="cb130-6"><a href="assumptions.html#cb130-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>)</span>
<span id="cb130-7"><a href="assumptions.html#cb130-7"></a></span>
<span id="cb130-8"><a href="assumptions.html#cb130-8"></a><span class="co"># Scatterplot of the residuals versus X</span></span>
<span id="cb130-9"><a href="assumptions.html#cb130-9"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">1</span>, <span class="kw">aes</span>(<span class="dt">x =</span> homework, <span class="dt">y =</span> .resid)) <span class="op">+</span></span>
<span id="cb130-10"><a href="assumptions.html#cb130-10"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb130-11"><a href="assumptions.html#cb130-11"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb130-12"><a href="assumptions.html#cb130-12"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb130-13"><a href="assumptions.html#cb130-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Time spent on homework (in hours)&quot;</span>) <span class="op">+</span></span>
<span id="cb130-14"><a href="assumptions.html#cb130-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-13-1.png" alt="LEFT: Density plot of the marginal distribution of residuals from the fitted regression model. RIGHT: Scatterplot of the residuals versus time spent on homework. The $Y=0$ line has been included as a reference for interpretation." width="45%%" /><img src="11-assumptions_files/figure-html/unnamed-chunk-13-2.png" alt="LEFT: Density plot of the marginal distribution of residuals from the fitted regression model. RIGHT: Scatterplot of the residuals versus time spent on homework. The $Y=0$ line has been included as a reference for interpretation." width="45%%" />
<p class="caption">
Figure 14: LEFT: Density plot of the marginal distribution of residuals from the fitted regression model. RIGHT: Scatterplot of the residuals versus time spent on homework. The <span class="math inline">\(Y=0\)</span> line has been included as a reference for interpretation.
</p>
</div>
<p>The marginal distribution of residuals looks symmetric and bell-shaped. Based on this plot, the normality assumption seems tenable (or at least there is negligable violation of the assumption). The scatterplot of residuals versus time spent on homework shows random scatter around the line <span class="math inline">\(Y=0\)</span>. This suggests that the average residual is roughly 0 at each <em>X</em>-value, and that the linearity assumption seems tenable. The range of the residuals at each <em>X</em>-value seems similar indicating that the assumption of equal variances is also tenable. Lastly, since the observations were randomly sampled (see <a href="http://zief0002.github.io/epsy-8251/codebooks/keith-gpa.html">data codebook</a>) we believe the independence assumption is satisfied.</p>
<p><br /></p>
</div>
<div id="standardized-residuals" class="section level2">
<h2>Standardized Residuals</h2>
<p>Often researchers standardize the residuals before performing the assumption checking. Using standardized residuals rather than unstandardized (raw) residuals does not change any of the previous findings. In fact, since standardizing is a linear transformation, the scatterplot and density plot look identical whether you use the unstandardized residuals or the standardized residuals. The only thing that changes is the scale on the residual axis of the plots.</p>
<p>To standardize a residual, we subtract the mean of its conditional distribution and divide by the standard deviation. The distributional assumptions stated that the mean of each conditional distribution of the residuals is 0 and the standard deviation is the same, although unknown, namely <span class="math inline">\(\sigma_{\epsilon}\)</span>. Mathematically, we standardize using:</p>
<p><span class="math display">\[
z_{\epsilon} = \frac{\epsilon - 0}{\mathrm{\sigma}_{\epsilon}}
\]</span></p>
<p>Unfortunately, we do not know what the value for <span class="math inline">\(\sigma_{\epsilon}\)</span>, so we need to estimate it from the data. This adds uncertainty to the calculation of the standardized residual in the same way estimating the standard error in a normal distribution adds uncertainty and makes the distribution <em>t</em>-distributed. As such, we write the formula for the standardized residuals using a <em>t</em> rather than <em>z</em> and compute it as:</p>
<p><span class="math display">\[
t_{\epsilon} = \frac{\epsilon - 0}{\mathrm{\hat\sigma}_{\epsilon}}
\]</span></p>
<p>Since the <em>t</em>-distribution is also referred to as <em>Student’s distribution</em>, standardized residuals are also sometimes referred to as <em>studentized residuals</em>.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> What standardizing does for us is to put the residuals on a scale that uses the standard error. This allows us to judge whether particular residuals that look extreme (either highly positive or negative) are actually extreme or not. The standardized residuals are given in the augmented output in the <code>.std.resid</code> column.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="assumptions.html#cb131-1"></a><span class="co"># Density plot of the residuals</span></span>
<span id="cb131-2"><a href="assumptions.html#cb131-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">1</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb131-3"><a href="assumptions.html#cb131-3"></a><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>) <span class="op">+</span></span>
<span id="cb131-4"><a href="assumptions.html#cb131-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb131-5"><a href="assumptions.html#cb131-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Standardized residual&quot;</span>) <span class="op">+</span></span>
<span id="cb131-6"><a href="assumptions.html#cb131-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>)</span>
<span id="cb131-7"><a href="assumptions.html#cb131-7"></a></span>
<span id="cb131-8"><a href="assumptions.html#cb131-8"></a><span class="co"># Scatterplot of the residuals versus X</span></span>
<span id="cb131-9"><a href="assumptions.html#cb131-9"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">1</span>, <span class="kw">aes</span>(<span class="dt">x =</span> homework, <span class="dt">y =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb131-10"><a href="assumptions.html#cb131-10"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb131-11"><a href="assumptions.html#cb131-11"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb131-12"><a href="assumptions.html#cb131-12"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb131-13"><a href="assumptions.html#cb131-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Time spent on homework (in hours)&quot;</span>) <span class="op">+</span></span>
<span id="cb131-14"><a href="assumptions.html#cb131-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-14-1.png" alt="LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The $Y=0$ line has been included as a reference for interpretation." width="45%%" /><img src="11-assumptions_files/figure-html/unnamed-chunk-14-2.png" alt="LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The $Y=0$ line has been included as a reference for interpretation." width="45%%" />
<p class="caption">
Figure 11: LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The <span class="math inline">\(Y=0\)</span> line has been included as a reference for interpretation.
</p>
</div>
<p>The only thing that has changed between these plots and the previous plots of the unstandardized residuals is the scale. However, now we can identify observations with extreme residuals, because we can make use of the fact that most of the residuals (~95%) should fall within two standard errors from the mean of 0. There are four students who have residuals of more than two standard errors. Given that we have <span class="math inline">\(N=100\)</span> observations, it is not surprising to see four observations more extreme than two standard errors; remember we expect to see 5% just by random chance. If observations have really extreme residuals (e.g., <span class="math inline">\(|t_{\epsilon}|&gt;3.5\)</span>), it is often worth a second look since these extreme observations are interesting and may point to something going on in the data.</p>
<p>We can also <code>filter()</code> the augmented data to find these observations and to determine the exact value of the standardized residuals. Recall that the vertical line (<code>|</code>) means “OR”.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="assumptions.html#cb132-1"></a>out_<span class="dv">1</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb132-2"><a href="assumptions.html#cb132-2"></a><span class="st">  </span><span class="kw">filter</span>(.std.resid <span class="op">&lt;=</span><span class="st"> </span><span class="dv">-2</span> <span class="op">|</span><span class="st"> </span>.std.resid <span class="op">&gt;=</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<pre><code># A tibble: 4 x 8
    gpa homework .fitted .resid .std.resid   .hat .sigma .cooksd
  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
1    64        7    82.8   18.8      -2.62 0.0187   7.02  0.0655
2    67        7    82.8   15.8      -2.20 0.0187   7.09  0.0462
3    64        5    80.4   16.4      -2.27 0.0100   7.08  0.0261
4   100        7    82.8  -17.2       2.40 0.0187   7.06  0.0549</code></pre>
<p><br /></p>
</div>
<div id="distributional-assumptions-for-the-multiple-regression-model" class="section level2">
<h2>Distributional Assumptions for the Multiple Regression Model</h2>
<p>Recall that the model for a multiple regression (with two predictors) is a plane that is fitted using observations composed of ordered triples, <span class="math inline">\((X_1,X_2,Y)\)</span>. Figure 8 visually shows the multiple regression model’s assumptions.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="figs/notes-11-multiple-regression-assumptions.png" alt="A visual depiction of the multiple regression model's assumptions." width="60%" />
<p class="caption">
Figure 8: A visual depiction of the multiple regression model’s assumptions.
</p>
</div>
<p>Now the conditional distributions that we put the assumptions on are the residuals (or <em>Y</em>-values) at each combination of (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(x_2\)</span>). The assumptions for the multiple regression model are similar to those for the simple model, namely,</p>
<ul>
<li><strong>Linearity:</strong> Notice from the visual that the MEAN values of each combination (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>) are linear in both the <span class="math inline">\(X_1\)</span> and the <span class="math inline">\(X_2\)</span> directions. This implies that the mean of each of the conditional distributions of residuals is zero at (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>).</li>
<li><strong>Independence:</strong> Again, this is not shown in the figure. The assumption is that each residual value in a particular conditional distribution is independent from every other residual value in that same distribution.</li>
<li><strong>Normality:</strong> This assumption indicates that each of the conditional distributions of residuals is normally distributed.</li>
<li><strong>Homoskedasticity:</strong> The variance (or standard deviation) of all of the conditional distributions of residuals is exactly the same.</li>
</ul>
<p>To evaluate these assumptions, we will create the exact same plots we created to evaluate the assumptions in the simple regression model, with one twist. Rather than creating the scatterplot by plotting the standardized residuals versus the <em>X</em>-value, we will plot them against the FITTED values (i.e., the <span class="math inline">\(\hat{Y}_i\)</span> values). The fitted values from a multiple regression represent the weighted combination of both predictors, and thus give us the appropriate conditioning when we examine the distributions. (Remember, we want to consider the distribution of residuals at each (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>) combination.)</p>
<p>As an example, we will regress student GPAs on both time spent on homework and parent education levels.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="assumptions.html#cb134-1"></a><span class="co"># Fit the multiple regression model</span></span>
<span id="cb134-2"><a href="assumptions.html#cb134-2"></a>lm<span class="fl">.2</span> =<span class="st"> </span><span class="kw">lm</span>(gpa <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>homework <span class="op">+</span><span class="st"> </span>parent_ed, <span class="dt">data =</span> keith)</span>
<span id="cb134-3"><a href="assumptions.html#cb134-3"></a></span>
<span id="cb134-4"><a href="assumptions.html#cb134-4"></a><span class="co"># Augment the model to obtain the fitted values and residuals</span></span>
<span id="cb134-5"><a href="assumptions.html#cb134-5"></a>out_<span class="dv">2</span> =<span class="st"> </span><span class="kw">augment</span>(lm<span class="fl">.2</span>)</span>
<span id="cb134-6"><a href="assumptions.html#cb134-6"></a><span class="kw">head</span>(out_<span class="dv">2</span>)</span></code></pre></div>
<pre><code># A tibble: 6 x 9
    gpa homework parent_ed .fitted .resid .std.resid   .hat .sigma  .cooksd
  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
1    78        2        13    76.5  -1.48      0.212 0.0330   7.13 0.000512
2    79        6        14    81.3   2.34     -0.332 0.0122   7.12 0.000454
3    79        1        13    75.5  -3.47      0.502 0.0500   7.12 0.00441 
4    89        5        13    79.5  -9.52      1.35  0.0130   7.06 0.00801 
5    82        3        16    80.1  -1.88      0.270 0.0390   7.13 0.000988
6    77        4        13    78.5   1.50     -0.213 0.0145   7.13 0.000221</code></pre>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="assumptions.html#cb136-1"></a><span class="co"># Density plot of the studentized residuals</span></span>
<span id="cb136-2"><a href="assumptions.html#cb136-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb136-3"><a href="assumptions.html#cb136-3"></a><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;#c62f4b&quot;</span>) <span class="op">+</span></span>
<span id="cb136-4"><a href="assumptions.html#cb136-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb136-5"><a href="assumptions.html#cb136-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Standardized residual&quot;</span>) <span class="op">+</span></span>
<span id="cb136-6"><a href="assumptions.html#cb136-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>)</span>
<span id="cb136-7"><a href="assumptions.html#cb136-7"></a></span>
<span id="cb136-8"><a href="assumptions.html#cb136-8"></a><span class="co"># Plot the standardized residuals versus the fitted values</span></span>
<span id="cb136-9"><a href="assumptions.html#cb136-9"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb136-10"><a href="assumptions.html#cb136-10"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb136-11"><a href="assumptions.html#cb136-11"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb136-12"><a href="assumptions.html#cb136-12"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb136-13"><a href="assumptions.html#cb136-13"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb136-14"><a href="assumptions.html#cb136-14"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span></span>
<span id="cb136-15"><a href="assumptions.html#cb136-15"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-17-1.png" alt="LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The $Y=0$, $Y=-2$ and $Y=2$ lines have been included as a references for interpretation." width="45%" /><img src="11-assumptions_files/figure-html/unnamed-chunk-17-2.png" alt="LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The $Y=0$, $Y=-2$ and $Y=2$ lines have been included as a references for interpretation." width="45%" />
<p class="caption">
Figure 15: LEFT: Density plot of the marginal distribution of standardized residuals from the fitted regression model. RIGHT: Scatterplot of the standardized residuals versus time spent on homework. The <span class="math inline">\(Y=0\)</span>, <span class="math inline">\(Y=-2\)</span> and <span class="math inline">\(Y=2\)</span> lines have been included as a references for interpretation.
</p>
</div>
<p>The density plot of the marginal distribution of standardized residuals is unimodal and roughly symmetric. There is deviation from normality, although this deviation seems minor and we note that the assumption of normality is robust to minor violations. The scatterplot shows random scatter around the <span class="math inline">\(Y=0\)</span> line which indicates that the mean residual value is close to 0 (linearity) for all fitted values. The range of the standardized residuals at each fitted value also seem roughly the same indicating that the equal variances assumption is also tenable. Lastly, since the observations were randomly sampled we believe the independence assumption is satisfied.</p>
<p>If any of the assumptions (aside from the independence assumption) do not seem reasonably satisfied, you can re-plot the residual plots based on the different simple regression models. (In this case we would look at the residuals versus time spent on homework and then the residuals versus parent education). This might help you identify if one, or both. of the predictors is the cause of the problem.</p>
<p>The reference lines in the scatterplot at <span class="math inline">\(Y=-2\)</span> and <span class="math inline">\(Y=2\)</span> help us identify observations with extreme residuals. (For large sample sizes these reference lines can be placed at <span class="math inline">\(Y=-3\)</span> and <span class="math inline">\(Y=3\)</span>.) There are a few observations that have residuals that are more than two standard errors from the mean. This indicates students that have relatively high (positive residual) or low (negative residual) GPAs given the time they spend each week on homework and their parent education level.</p>
<p><br /></p>
</div>
<div id="regression-model-revisited" class="section level2">
<h2>Regression Model Revisited</h2>
<p>To this point, we have been writing the regression model as a mathematical expression of the relationship between some outcome (<span class="math inline">\(Y\)</span>) and a set of predictors (<span class="math inline">\(X_1,X_2,\ldots,X_k\)</span>), namely as,</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1(X_{1i}) + \beta_2(X_{2i}) + \ldots + \beta_k(X_{ki}) + \epsilon_i
\]</span></p>
<p>This is partially correct. A statistical model needs to represent the data generating process, which also embodies the set of underlying distributional assumptions. This implies that when we write out the regression model, it should include the mathematical relationship and the underlying distributional assumptions.</p>
<p><span class="math display">\[
\begin{gathered}
Y_i = \beta_0 + \beta_1(X_{1i}) + \beta_2(X_{2i}) + \ldots + \beta_k(X_{ki}) + \epsilon_i \\[2ex]
\mathrm{where}~\quad\epsilon_{i|X} \overset{\mathrm{i.i.d~}}{\sim}  \mathcal{N}\left(0, \sigma^2\right)
\end{gathered}
\]</span></p>
<p><br /></p>
</div>
<div id="advanced-plotting-accounting-for-sampling-uncertainty-in-the-density-plot" class="section level2">
<h2>Advanced Plotting: Accounting for Sampling Uncertainty in the Density Plot</h2>
<p>So far, when we have evaluated the normality assumption we have relied on our intuition (and experience) about whether the density plot of the marginal distribution of residuals was close to (or at least close enough to) normal. One thing we could do to help with this evaluation is to include a reference line showing normality as a basis of comparison. For example, here we again plot the marginal distribution of the standardized residuals from the fitted multiple regression. But this time, we also include the normal reference density.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="assumptions.html#cb137-1"></a><span class="co"># Density plot of the standardized residuals</span></span>
<span id="cb137-2"><a href="assumptions.html#cb137-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb137-3"><a href="assumptions.html#cb137-3"></a><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;#c62f4b&quot;</span>) <span class="op">+</span></span>
<span id="cb137-4"><a href="assumptions.html#cb137-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>), <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb137-5"><a href="assumptions.html#cb137-5"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb137-6"><a href="assumptions.html#cb137-6"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Standardized residual&quot;</span>) <span class="op">+</span></span>
<span id="cb137-7"><a href="assumptions.html#cb137-7"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-18-1.png" alt="Density plot of the marginal distribution of standardized residuals from the fitted regression model (raspberry, solid line). The density for a ~N(0,1) distribution (black, dashed line) have been included as a comparative reference." width="50%" />
<p class="caption">
Figure 17: Density plot of the marginal distribution of standardized residuals from the fitted regression model (raspberry, solid line). The density for a ~N(0,1) distribution (black, dashed line) have been included as a comparative reference.
</p>
</div>
<p>The <code>stat_function()</code> layer uses the <code>dnorm()</code> function to plot the normal density. Since this is for the standardized residuals, we assume a mean of 0 and a standard deviation of 1 for this normal distribution. These parameters are included in the <code>args=list()</code> argument. The density of the standardized residuals is close to that for the normal distribution, although it is not perfect (e.g., flatter peak than normal). The big question is whether this deviation is more than we would expect because of sampling error?</p>
<p>To answer this question, we need to be able to visualize the uncertainty that is due to sampling error. To include</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-19-1.png" alt="The density for a ~N(0,1) distribution (black, dashed line) and the sampling uncertainty associated with that normal distribution (blue shaded area)." width="50%" />
<p class="caption">
Figure 16: The density for a ~N(0,1) distribution (black, dashed line) and the sampling uncertainty associated with that normal distribution (blue shaded area).
</p>
</div>
<p>In this plot, the black, dashed line corresponds to where the density curve would lie if the distribution was normally distributed. The blue shaded area is the confidence envelope for the normal distribution. In other words, it shows the area we would expect a density curve to lie in if it came from the normal distribution.</p>
<p>To create this confidence envelope, we will use the <code>stat_density_confidence()</code> layer from the <strong>educate</strong> package (see below for instructions on how to install the <strong>educate</strong> package).</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="assumptions.html#cb138-1"></a><span class="co"># Load library</span></span>
<span id="cb138-2"><a href="assumptions.html#cb138-2"></a><span class="kw">library</span>(educate)</span>
<span id="cb138-3"><a href="assumptions.html#cb138-3"></a></span>
<span id="cb138-4"><a href="assumptions.html#cb138-4"></a><span class="co"># Density plot of the studentized residuals</span></span>
<span id="cb138-5"><a href="assumptions.html#cb138-5"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb138-6"><a href="assumptions.html#cb138-6"></a><span class="st">  </span><span class="kw">stat_density_confidence</span>() <span class="op">+</span></span>
<span id="cb138-7"><a href="assumptions.html#cb138-7"></a><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;#c62f4b&quot;</span>) <span class="op">+</span></span>
<span id="cb138-8"><a href="assumptions.html#cb138-8"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb138-9"><a href="assumptions.html#cb138-9"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Standardized residual&quot;</span>) <span class="op">+</span></span>
<span id="cb138-10"><a href="assumptions.html#cb138-10"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Probability density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-20-1.png" alt="Density plot of the marginal distribution of standardized residuals from the fitted regression model (raspberry line). The sampling uncertainty associated with the normal distribution is also displayed (blue shaded area)." width="50%" />
<p class="caption">
Figure 18: Density plot of the marginal distribution of standardized residuals from the fitted regression model (raspberry line). The sampling uncertainty associated with the normal distribution is also displayed (blue shaded area).
</p>
</div>
<p>The raspberry line depicting the density of the marginal distribution of standardized residuals lies completely within the blue area. This suggests that the deviation we saw earlier from the normal distribution is consistent with just being sampling error. Thus we conclude that the normality assumption is tenable.</p>
<p><br /></p>
<div id="installing-the-educate-package" class="section level3">
<h3>Installing the educate Package</h3>
<p>The <strong>educate</strong> package is not available on CRAN, thus you cannot install it using the <code>Install</code> button in RStudio. To install this package, we need to use the <code>install_github()</code> function from the <strong>remotes</strong> package.</p>
<ol style="list-style-type: decimal">
<li>Use the <code>Install</code> button in RStudio to install the <strong>remotes</strong> package.</li>
<li>Once the <strong>remotes</strong> package has successfully installed, use the following syntax to install the <strong>educate</strong> package:</li>
</ol>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="assumptions.html#cb139-1"></a>remotes<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;zief0002/educate&quot;</span>)</span></code></pre></div>
<p><br /></p>
</div>
</div>
<div id="advanced-plotting-loess-smooth-to-help-evaluate-linearity" class="section level2">
<h2>Advanced Plotting: Loess Smooth to Help Evaluate Linearity</h2>
<p>In the scatterplot of the standardized residuals versus the fitted values, we would expect that the average value of the residual at a given fitted value would be 0. The loess smoother helps us visualize the mean pattern in the actual data. We can then compare this to what would be expected (a constant mean pattern of 0) to evaluate the linearity assumption.</p>
<p>To add a loess smoother, we use the <code>geom_smooth()</code> function with the argument <code>method="loess"</code>. This will plot the loess line and also the confidence envelope around that loess line. This gives us an indication of the mean pattern in the data and its uncertainty. We would hope to see the line <span class="math inline">\(Y=0\)</span> (our expected pattern under linearity) encompassed in the uncertainty.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="assumptions.html#cb140-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb140-2"><a href="assumptions.html#cb140-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb140-3"><a href="assumptions.html#cb140-3"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>) <span class="op">+</span></span>
<span id="cb140-4"><a href="assumptions.html#cb140-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb140-5"><a href="assumptions.html#cb140-5"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb140-6"><a href="assumptions.html#cb140-6"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span></span>
<span id="cb140-7"><a href="assumptions.html#cb140-7"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-22-1.png" alt="Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. A horizontal line at $Y=0$ shows the expected mean residual under the linearity assumption. The loess line (blue) and uncertainty bands (grey shaded area) are also displayed." width="50%" />
<p class="caption">
Figure 21: Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. A horizontal line at <span class="math inline">\(Y=0\)</span> shows the expected mean residual under the linearity assumption. The loess line (blue) and uncertainty bands (grey shaded area) are also displayed.
</p>
</div>
<p>Note that the loess line shows some deviation from the <span class="math inline">\(Y=0\)</span> line, however this is likely just due to sampling variation as the line <span class="math inline">\(Y=0\)</span> is encompassed in the confidence envelope. Because of this we would suggest that the linearity assumption is tenable.</p>
<p><br /></p>
</div>
<div id="advanced-plotting-identify-observations-with-extreme-residuals" class="section level2">
<h2>Advanced Plotting: Identify Observations with Extreme Residuals</h2>
<p>It can be useful to identify particular observations in the residual plots directly. This can be useful as you explore the plots, and also to create plots for publications in which you wish to highlight particular cases. Rather than plotting points (<code>geom_point()</code>) for each observation, we can plot text for each observation using <code>geom_text()</code>. For example, you might imagine writing the name of each student in place of their point on the scatterplot. To do this, we need to:</p>
<ol style="list-style-type: decimal">
<li>Create an ID variable in the augmented data.</li>
<li>Use <code>geom_text()</code> rather than <code>geom_point()</code> in the ggplot syntax. In the <code>geom_text()</code> function we will set <code>label=</code> to the newly created ID variable, and since it is a variable in the data set, we will put that in an <code>aes()</code> function.</li>
</ol>
<p>Since the original data set does not include an ID variable (e.g., names), we will use the row number from the original data as the ID. In other words the student in the first row will have an ID of 1, the student in the second row will have an ID of 2, etc.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="assumptions.html#cb141-1"></a><span class="co"># Create ID variable in the augmented data</span></span>
<span id="cb141-2"><a href="assumptions.html#cb141-2"></a>out_<span class="dv">2</span> =<span class="st"> </span>out_<span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb141-3"><a href="assumptions.html#cb141-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id =</span> <span class="kw">row.names</span>(keith))</span>
<span id="cb141-4"><a href="assumptions.html#cb141-4"></a></span>
<span id="cb141-5"><a href="assumptions.html#cb141-5"></a><span class="co"># View new data</span></span>
<span id="cb141-6"><a href="assumptions.html#cb141-6"></a><span class="kw">head</span>(out_<span class="dv">2</span>)</span></code></pre></div>
<pre><code># A tibble: 6 x 10
    gpa homework parent_ed .fitted .resid .std.resid   .hat .sigma .cooksd id   
  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;
1    78        2        13    76.5  -1.48      0.212 0.0330   7.13 5.12e-4 1    
2    79        6        14    81.3   2.34     -0.332 0.0122   7.12 4.54e-4 2    
3    79        1        13    75.5  -3.47      0.502 0.0500   7.12 4.41e-3 3    
4    89        5        13    79.5  -9.52      1.35  0.0130   7.06 8.01e-3 4    
5    82        3        16    80.1  -1.88      0.270 0.0390   7.13 9.88e-4 5    
6    77        4        13    78.5   1.50     -0.213 0.0145   7.13 2.21e-4 6    </code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="assumptions.html#cb143-1"></a><span class="co"># Plot the id variable as text rather than points in the scatterplot</span></span>
<span id="cb143-2"><a href="assumptions.html#cb143-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> out_<span class="dv">2</span>, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb143-3"><a href="assumptions.html#cb143-3"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> id), <span class="dt">size =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb143-4"><a href="assumptions.html#cb143-4"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb143-5"><a href="assumptions.html#cb143-5"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb143-6"><a href="assumptions.html#cb143-6"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">-2</span>, <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>) <span class="op">+</span></span>
<span id="cb143-7"><a href="assumptions.html#cb143-7"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">2</span>, <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>) <span class="op">+</span></span>
<span id="cb143-8"><a href="assumptions.html#cb143-8"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span></span>
<span id="cb143-9"><a href="assumptions.html#cb143-9"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-23"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-23-1.png" alt="Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. The values plotted indicate the students' row numbers in the data. A horizontal line at $Y=0$ is included to aid interpretation." width="50%" />
<p class="caption">
Figure 22: Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. The values plotted indicate the students’ row numbers in the data. A horizontal line at <span class="math inline">\(Y=0\)</span> is included to aid interpretation.
</p>
</div>
<p>We can also plot points for some students and ID label for other students. For example, suppose we wanted to give the ID number for only those students with a standardized residual that was less than <span class="math inline">\(-2\)</span> or greater than 2, and plot a point otherwise. To do this, we would create the ID variable in the augmented data (which we have already done), then split the data frame into two data frames: one for those students with extreme residuals and one for those that have a non-extreme residual. Then we will call <code>geom_point()</code> for those in the non-extreme data set, and <code>geom_text()</code> for those in the extreme set. We do this by including a <code>data=</code> argument in one of those functions to reference a different data frame.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="assumptions.html#cb144-1"></a><span class="co"># Create different data sets for the extreme and non-extreme observations</span></span>
<span id="cb144-2"><a href="assumptions.html#cb144-2"></a>extreme =<span class="st"> </span>out_<span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb144-3"><a href="assumptions.html#cb144-3"></a><span class="st">  </span><span class="kw">filter</span>(.std.resid <span class="op">&lt;=</span><span class="st"> </span><span class="dv">-2</span> <span class="op">|</span><span class="st"> </span>.std.resid <span class="op">&gt;=</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb144-4"><a href="assumptions.html#cb144-4"></a></span>
<span id="cb144-5"><a href="assumptions.html#cb144-5"></a>nonextreme =<span class="st"> </span>out_<span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb144-6"><a href="assumptions.html#cb144-6"></a><span class="st">  </span><span class="kw">filter</span>(.std.resid <span class="op">&gt;</span><span class="st"> </span><span class="dv">-2</span> <span class="op">&amp;</span><span class="st"> </span>.std.resid <span class="op">&lt;</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb144-7"><a href="assumptions.html#cb144-7"></a></span>
<span id="cb144-8"><a href="assumptions.html#cb144-8"></a><span class="co"># Plot using text for the extreme observations and points for the non-extreme</span></span>
<span id="cb144-9"><a href="assumptions.html#cb144-9"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> extreme, <span class="kw">aes</span>(<span class="dt">x =</span> .fitted, <span class="dt">y =</span> .std.resid)) <span class="op">+</span></span>
<span id="cb144-10"><a href="assumptions.html#cb144-10"></a><span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> id), <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb144-11"><a href="assumptions.html#cb144-11"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> nonextreme) <span class="op">+</span></span>
<span id="cb144-12"><a href="assumptions.html#cb144-12"></a><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb144-13"><a href="assumptions.html#cb144-13"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb144-14"><a href="assumptions.html#cb144-14"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span></span>
<span id="cb144-15"><a href="assumptions.html#cb144-15"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Standardized residual&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-24"></span>
<img src="11-assumptions_files/figure-html/unnamed-chunk-24-1.png" alt="Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. Students with standardized residuals more than two standard errors from 0 are identified by their row number. A horizontal line at $Y=0$ is included to aid interpretation." width="50%" />
<p class="caption">
Figure 23: Scatterplot of the standardized residuals versus the fitted values from a regression model using time spent on homework and parent education level to predict GPA. Students with standardized residuals more than two standard errors from 0 are identified by their row number. A horizontal line at <span class="math inline">\(Y=0\)</span> is included to aid interpretation.
</p>
</div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>This is true for other statistical models as well (e.g., ANOVA, <span class="math inline">\(t\)</span>-test).<a href="assumptions.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>We cover some of these methods in EPsy 8252.<a href="assumptions.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>We will cover some of those transformations in EPsy 8252.<a href="assumptions.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>Again, there are many transformations of the data that can alleviate this problem.<a href="assumptions.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>Technically they are <em>internally studentized residuals</em>.<a href="assumptions.html#fnref17" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="statcontrol.html"><button class="btn btn-default">Previous</button></a>
<a href="references.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
